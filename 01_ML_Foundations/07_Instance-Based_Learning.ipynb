{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Instance-Based Learning\n",
    "\n",
    "### Definition\n",
    "**Instance-Based Learning** (also called **Lazy Learning** or **Memory-Based Learning**) is an approach where the model:\n",
    "1. **Stores** the entire training dataset in memory\n",
    "2. **Learns** during prediction phase, not training phase\n",
    "3. **Makes predictions** by comparing new instances to stored examples using similarity measures\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "```\n",
    "Training Phase:\n",
    "Input Data \u2192 Store exactly as is \u2192 Done!\n",
    "(No model creation, no parameter learning)\n",
    "\n",
    "Prediction Phase:\n",
    "New Sample \u2192 Find similar stored examples \u2192 Predict based on neighbors\n",
    "```\n",
    "\n",
    "### How It Works: K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is the most popular instance-based algorithm.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Store all training examples\n",
    "2. For new sample to predict:\n",
    "   a. Calculate distance to all stored examples\n",
    "   b. Find K nearest neighbors\n",
    "   c. Return class (majority vote) or value (average) of K neighbors\n",
    "\n",
    "#### Distance Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"Straight-line distance between two points\"\"\"\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def manhattan_distance(x1, x2):\n",
    "    \"\"\"Taxicab distance (sum of absolute differences)\"\"\"\n",
    "    return np.sum(np.abs(x1 - x2))\n",
    "\n",
    "def cosine_similarity(x1, x2):\n",
    "    \"\"\"Angle-based similarity (useful for text/documents)\"\"\"\n",
    "    dot_product = np.dot(x1, x2)\n",
    "    norms = np.linalg.norm(x1) * np.linalg.norm(x2)\n",
    "    return dot_product / norms if norms > 0 else 0\n",
    "\n",
    "# Example\n",
    "point1 = np.array([0, 0])\n",
    "point2 = np.array([3, 4])\n",
    "\n",
    "print(f\"Euclidean distance: {euclidean_distance(point1, point2):.2f}\")  # 5\n",
    "print(f\"Manhattan distance: {manhattan_distance(point1, point2):.2f}\")  # 7\n",
    "print(f\"Cosine similarity: {cosine_similarity(point1, point2):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Classification Example\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# KNN Classification\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train, y_train)  # Stores training data, doesn't build model\n",
    "predictions = knn_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"KNN Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Regression Example\n",
    "diabetes = load_diabetes()\n",
    "X_d, y_d = diabetes.data, diabetes.target\n",
    "\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_d, y_d, test_size=0.2\n",
    ")\n",
    "\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_reg.fit(X_train_d, y_train_d)\n",
    "predictions_reg = knn_reg.predict(X_test_d)\n",
    "mse = mean_squared_error(y_test_d, predictions_reg)\n",
    "print(f\"KNN Regression MSE: {mse:.4f}\")\n",
    "\n",
    "# Visualizing KNN decision boundary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create simple 2D dataset\n",
    "X_2d = X_train[:, :2]  # Use only first 2 features for visualization\n",
    "\n",
    "# Create mesh for decision boundary\n",
    "h = 0.02  # Step size\n",
    "x_min, x_max = X_2d[:, 0].min() - 0.5, X_2d[:, 0].max() + 0.5\n",
    "y_min, y_max = X_2d[:, 1].min() - 0.5, X_2d[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                      np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict on mesh\n",
    "Z = knn_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_train[:len(X_2d)], \n",
    "            cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "plt.title('KNN Decision Boundary (K=5)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Instance-Based Learning\n",
    "\n",
    "#### 1. **Non-Parametric**\n",
    "- No explicit model parameters to learn\n",
    "- Model complexity grows with training data\n",
    "- Can adapt to any complex pattern\n",
    "- No assumptions about data distribution\n",
    "\n",
    "#### 2. **Lazy Learning**\n",
    "- No learning phase: just stores data\n",
    "- Actual computation happens at prediction time\n",
    "- Fast training (just copy data)\n",
    "- Slow prediction (calculate distances to all samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Training phase: Very fast (just stores data)\n",
    "start = time.time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)  # Just stores X_train!\n",
    "train_time = time.time() - start\n",
    "print(f\"Training time: {train_time:.4f} seconds (very fast)\")\n",
    "\n",
    "# Prediction phase: Slower (calculate distances)\n",
    "start = time.time()\n",
    "predictions = knn.predict(X_test)\n",
    "predict_time = time.time() - start\n",
    "print(f\"Prediction time: {predict_time:.4f} seconds (slower)\")\n",
    "print(f\"Ratio: Prediction is {predict_time/train_time:.0f}x slower than training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Flexible**\n",
    "- Quickly adapts to new data\n",
    "- Can learn complex non-linear patterns\n",
    "- Works with any distance metric\n",
    "- No retraining needed for new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new data: Simply add to memory\n",
    "new_X = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "new_y = np.array([0])\n",
    "\n",
    "# Just concatenate - no retraining!\n",
    "X_updated = np.vstack([X_train, new_X])\n",
    "y_updated = np.hstack([y_train, new_y])\n",
    "\n",
    "knn_updated = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_updated.fit(X_updated, y_updated)  # Fast update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Instance-Based Learning\n",
    "\n",
    "#### 1. **Simple Implementation** \u2705\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation is straightforward\n",
    "# Just calculate distance and find nearest neighbors\n",
    "\n",
    "def simple_knn_predict(X_train, y_train, x_new, k=5):\n",
    "    \"\"\"Simple KNN implementation\"\"\"\n",
    "    distances = []\n",
    "    for x_train_sample in X_train:\n",
    "        dist = np.sqrt(np.sum((x_train_sample - x_new) ** 2))\n",
    "        distances.append(dist)\n",
    "    \n",
    "    # Find K nearest\n",
    "    distances = np.array(distances)\n",
    "    k_nearest_indices = np.argsort(distances)[:k]\n",
    "    k_nearest_labels = y_train[k_nearest_indices]\n",
    "    \n",
    "    # Majority vote\n",
    "    prediction = np.bincount(k_nearest_labels).argmax()\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Quick Adaptation** \ud83d\udd04\n",
    "- New data incorporated immediately\n",
    "- No need for full retraining\n",
    "- Perfect for streaming data\n",
    "\n",
    "#### 3. **Handles Complex Patterns** \ud83c\udfaf\n",
    "- No assumptions about underlying distribution\n",
    "- Captures non-linear relationships\n",
    "- Works with high-dimensional data\n",
    "- Flexible decision boundaries\n",
    "\n",
    "#### 4. **Naturally Handles Multi-Class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN naturally works with multiple classes\n",
    "knn_multiclass = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_multiclass.fit(X_train, y_train)  # Works with any number of classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Good for Small Datasets** \ud83d\udcca\n",
    "- Works well when training data is limited\n",
    "- Each example is valuable, Instance-based uses all\n",
    "- Better than model-based which might overfit\n",
    "\n",
    "### Disadvantages of Instance-Based Learning\n",
    "\n",
    "#### 1. **High Memory Requirement** \u274c\n",
    "- Must store entire training dataset\n",
    "- Large datasets require huge memory\n",
    "- Not suitable for millions of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory problem with large datasets\n",
    "import sys\n",
    "\n",
    "# Store 1 million samples with 100 features\n",
    "X_large = np.random.rand(1_000_000, 100)\n",
    "print(f\"Memory required: {X_large.nbytes / (1024**3):.1f} GB\")\n",
    "# Requires ~800GB for just features!\n",
    "\n",
    "# KNN must keep all in memory\n",
    "knn_large = KNeighborsClassifier()\n",
    "knn_large.fit(X_large, y_large)\n",
    "# Cannot fit on standard hardware!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Slow Prediction** \ud83d\udc22\n",
    "- Must calculate distance to ALL training examples\n",
    "- Prediction time: O(n \u00d7 d) where n=samples, d=features\n",
    "- Becomes impractical for millions of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction speed degrades with dataset size\n",
    "dataset_sizes = [100, 1000, 10000, 100000]\n",
    "prediction_times = []\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    X_temp = np.random.rand(size, 50)\n",
    "    y_temp = np.random.randint(0, 2, size)\n",
    "    \n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_temp.fit(X_temp, y_temp)\n",
    "    \n",
    "    X_test_temp = np.random.rand(100, 50)\n",
    "    \n",
    "    start = time.time()\n",
    "    knn_temp.predict(X_test_temp)\n",
    "    elapsed = time.time() - start\n",
    "    prediction_times.append(elapsed)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dataset_sizes, prediction_times, marker='o', linewidth=2)\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Prediction Time (seconds)')\n",
    "plt.title('KNN Prediction Speed vs Dataset Size')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Sensitive to Irrelevant Features** \ud83c\udfb2\n",
    "- All features treated equally in distance calculation\n",
    "- Irrelevant features increase noise\n",
    "- Performance degrades in high dimensions (curse of dimensionality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curse of dimensionality\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Few dimensions: KNN works well\n",
    "X_2d, y_2d = make_classification(n_samples=500, n_features=2, \n",
    "                                 n_informative=2, n_redundant=0)\n",
    "knn_2d = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_2d.fit(X_2d, y_2d)\n",
    "acc_2d = knn_2d.score(X_2d, y_2d)\n",
    "print(f\"2D accuracy: {acc_2d:.4f}\")\n",
    "\n",
    "# Many dimensions: KNN struggles\n",
    "X_100d, y_100d = make_classification(n_samples=500, n_features=100,\n",
    "                                     n_informative=2, n_redundant=98)\n",
    "knn_100d = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_100d.fit(X_100d, y_100d)\n",
    "acc_100d = knn_100d.score(X_100d, y_100d)\n",
    "print(f\"100D accuracy: {acc_100d:.4f} (worse due to curse of dimensionality)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Sensitive to Noise** \ud83d\udd0a\n",
    "- Noisy data points become problematic \"neighbors\"\n",
    "- Outliers can mislead predictions\n",
    "- No learning process to filter noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of noise\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_clean, y_clean = make_classification(n_samples=200, n_features=20, \n",
    "                                       n_informative=10, noise=0)\n",
    "X_noisy, y_noisy = make_classification(n_samples=200, n_features=20,\n",
    "                                       n_informative=10, noise=10)\n",
    "\n",
    "# Split into train/test\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = \\\n",
    "    train_test_split(X_clean, y_clean, test_size=0.3)\n",
    "X_train_noisy, X_test_noisy, y_train_noisy, y_test_noisy = \\\n",
    "    train_test_split(X_noisy, y_noisy, test_size=0.3)\n",
    "\n",
    "# KNN on clean data\n",
    "knn_clean = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clean.fit(X_train_clean, y_train_clean)\n",
    "acc_clean = knn_clean.score(X_test_clean, y_test_clean)\n",
    "\n",
    "# KNN on noisy data\n",
    "knn_noisy = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_noisy.fit(X_train_noisy, y_train_noisy)\n",
    "acc_noisy = knn_noisy.score(X_test_noisy, y_test_noisy)\n",
    "\n",
    "print(f\"Clean data accuracy: {acc_clean:.4f}\")\n",
    "print(f\"Noisy data accuracy: {acc_noisy:.4f}\")\n",
    "print(f\"Accuracy drop: {(acc_clean - acc_noisy)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Imbalanced Data Sensitivity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority class dominates predictions\n",
    "# Need to handle carefully\n",
    "weighted_knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance'  # Weight by distance (closer neighbors matter more)\n",
    ")\n",
    "weighted_knn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Instance-Based Learning\n",
    "\n",
    "\u2705 **Use Instance-Based when:**\n",
    "- Small to medium dataset\n",
    "- Data patterns complex and non-linear\n",
    "- Quick training needed\n",
    "- Data constantly updating\n",
    "- Want simple, interpretable model\n",
    "- Limited labeled data (few-shot learning)\n",
    "\n",
    "\u274c **Avoid when:**\n",
    "- Millions of samples (memory/speed)\n",
    "- Real-time predictions critical\n",
    "- Storage is constrained\n",
    "- Noisy data\n",
    "- High-dimensional data (100+ features)\n",
    "\n",
    "### Real-World Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Medical Diagnosis\n",
    "# Find similar patient cases and use their outcomes\n",
    "\n",
    "# Example 2: Recommendation Systems\n",
    "# Find users with similar preferences\n",
    "\n",
    "# Example 3: Document Classification\n",
    "# Find similar documents to classify new ones\n",
    "\n",
    "class RecommendationSystem:\n",
    "    \"\"\"Simple content-based recommendation using KNN\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    def train(self, user_embeddings, item_ids):\n",
    "        \"\"\"Train on existing user-item interactions\"\"\"\n",
    "        self.knn.fit(user_embeddings, item_ids)\n",
    "    \n",
    "    def recommend(self, new_user_embedding, top_k=3):\n",
    "        \"\"\"Recommend items similar to new user\"\"\"\n",
    "        distances, indices = self.knn.kneighbors([new_user_embedding], n_neighbors=5)\n",
    "        return indices[0][:top_k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}