{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Rate\n",
    "\n",
    "### Definition\n",
    "**Learning Rate** is a hyperparameter that controls **how much model parameters change** in response to the calculated error during training. It determines the **step size** in the optimization process.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "In gradient descent optimization, the learning rate (typically denoted as \u03b7 or \u03b1) is part of the weight update formula:\n",
    "\n",
    "```\n",
    "w_new = w_old - \u03b7 \u00d7 \u2207L(w)\n",
    "\n",
    "Where:\n",
    "  w_old = current weight\n",
    "  \u03b7 = learning rate (step size)\n",
    "  \u2207L(w) = gradient of loss function (direction of steepest descent)\n",
    "  w_new = updated weight\n",
    "```\n",
    "\n",
    "### Intuitive Explanation\n",
    "\n",
    "Imagine hiking down a mountain trying to reach the valley (optimal solution):\n",
    "- **High Learning Rate:** Take big steps down the mountain\n",
    "  - Fast descent but might overshoot the valley\n",
    "  - Risk of missing the optimal point\n",
    "  - May diverge (go back up the mountain)\n",
    "- **Low Learning Rate:** Take tiny steps down the mountain\n",
    "  - Safer descent, likely to reach valley\n",
    "  - Takes forever to get there\n",
    "  - Convergence very slow\n",
    "- **Optimal Learning Rate:** Right-sized steps\n",
    "  - Efficiently reaches valley\n",
    "  - Doesn't overshoot\n",
    "  - Best training speed\n",
    "\n",
    "### Python Example: Learning Rate Impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate simple regression data\n",
    "X, y = make_regression(n_samples=500, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "# Test different learning rates\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "losses = {lr: [] for lr in learning_rates}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = SGDRegressor(\n",
    "        learning_rate='constant',\n",
    "        eta0=lr,  # Initial learning rate\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Track loss for each iteration\n",
    "    for i in range(100):\n",
    "        model.fit(X, y)\n",
    "        current_pred = model.predict(X)\n",
    "        mse_loss = np.mean((current_pred - y) ** 2)\n",
    "        losses[lr].append(mse_loss)\n",
    "\n",
    "# Visualize the effect\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.plot(losses[lr], marker='o', linewidth=2)\n",
    "    ax.set_title(f'Learning Rate = {lr}')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss (MSE)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Analyze convergence\n",
    "    final_loss = losses[lr][-1]\n",
    "    min_loss = min(losses[lr])\n",
    "    ax.text(0.5, 0.95, f'Final Loss: {final_loss:.4f}',\n",
    "            transform=ax.transAxes, ha='center', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_rate_impact.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Analysis of Learning Rates:\")\n",
    "for lr in learning_rates:\n",
    "    min_loss = min(losses[lr])\n",
    "    final_loss = losses[lr][-1]\n",
    "    converged = min_loss < final_loss * 1.05  # Roughly converged\n",
    "    print(f\"\u03b7={lr:7.4f}: Min Loss={min_loss:.4f}, Final Loss={final_loss:.4f}, \"\n",
    "          f\"Converged={converged}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Different Learning Rates\n",
    "\n",
    "#### Too Low Learning Rate (\u03b7 too small) \ud83d\udc0c\n",
    "```\n",
    "Problem: Convergence extremely slow\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- Takes thousands of iterations to converge\n",
    "- Model barely improves with each step\n",
    "- Training time becomes impractical\n",
    "- May get stuck in local minima\n",
    "\n",
    "**Code Example:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# Too low learning rate\n",
    "model = SGDClassifier(eta0=0.00001, learning_rate='constant', max_iter=1000)\n",
    "model.fit(X, y)\n",
    "accuracy_low_lr = model.score(X, y)\n",
    "print(f\"Accuracy with LR=0.00001: {accuracy_low_lr:.4f}\")\n",
    "# Likely still underfitting after 1000 iterations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Too High Learning Rate (\u03b7 too large) \ud83d\ude80\n",
    "```\n",
    "Problem: Model diverges or oscillates around optimal\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- Loss increases instead of decreasing\n",
    "- Parameters become NaN or explode in magnitude\n",
    "- Model diverges away from optimum\n",
    "- Updates are too drastic\n",
    "\n",
    "**Code Example:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too high learning rate\n",
    "model = SGDClassifier(eta0=10.0, learning_rate='constant', max_iter=100)\n",
    "model.fit(X, y)\n",
    "# Predictions might be all 0 or all 1 (completely wrong!)\n",
    "# Loss might actually increase with iterations\n",
    "\n",
    "# Check weight magnitude\n",
    "print(f\"Model weights (might be huge): {model.coef_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just Right Learning Rate (optimal \u03b7) \u2705\n",
    "```\n",
    "Fast convergence to near-optimal solution\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- Loss smoothly decreases\n",
    "- Converges in reasonable iterations (usually < 100)\n",
    "- Stable training process\n",
    "- Good final model performance\n",
    "\n",
    "### Factors Affecting Learning Rate\n",
    "\n",
    "#### 1. **Batch Size**\n",
    "Larger batches often tolerate higher learning rates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Smaller batch size \u2192 smaller learning rate needed\n",
    "model_small_batch = SGDClassifier(eta0=0.01, batch_size=32)\n",
    "model_small_batch.fit(X, y)\n",
    "\n",
    "# Larger batch size \u2192 can use larger learning rate\n",
    "model_large_batch = SGDClassifier(eta0=0.1, batch_size=256)\n",
    "model_large_batch.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Data Normalization**\n",
    "Pre-processed data requires different learning rates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Unnormalized data\n",
    "model1 = SGDClassifier(eta0=0.01)\n",
    "model1.fit(X, y)  # Might fail or be slow\n",
    "\n",
    "# Normalized data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "model2 = SGDClassifier(eta0=0.01)\n",
    "model2.fit(X_normalized, y)  # Works much better with same LR!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Optimization Algorithm**\n",
    "Different algorithms have different learning rate sensitivity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD: Simple, learning rate matters greatly\n",
    "model_sgd = SGDClassifier(eta0=0.1, learning_rate='constant')\n",
    "\n",
    "# Adam: Adaptive learning rates, less sensitive to initial LR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model_adam = MLPClassifier(learning_rate_init=0.001, solver='adam')\n",
    "model_adam.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedules\n",
    "\n",
    "Instead of fixed learning rate, reduce it over time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "class LearningRateScheduler:\n",
    "    def __init__(self, initial_lr=0.1, decay=0.9, schedule_type='exponential'):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.decay = decay\n",
    "        self.schedule_type = schedule_type\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.schedule_type == 'constant':\n",
    "            return self.initial_lr\n",
    "        \n",
    "        elif self.schedule_type == 'exponential':\n",
    "            # Exponential decay: \u03b7 = \u03b7\u2080 \u00d7 decay^iteration\n",
    "            return self.initial_lr * (self.decay ** self.iteration)\n",
    "        \n",
    "        elif self.schedule_type == 'linear':\n",
    "            # Linear decay: \u03b7 = \u03b7\u2080 \u00d7 (1 - iteration/max_iterations)\n",
    "            return self.initial_lr * (1 - self.iteration / 100)\n",
    "        \n",
    "        elif self.schedule_type == 'step':\n",
    "            # Step decay: reduce by factor every N iterations\n",
    "            step = self.iteration // 20\n",
    "            return self.initial_lr * (0.5 ** step)\n",
    "    \n",
    "    def step(self):\n",
    "        self.iteration += 1\n",
    "\n",
    "# Visualize different schedules\n",
    "scheduler_types = ['constant', 'exponential', 'linear', 'step']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for idx, schedule_type in enumerate(scheduler_types):\n",
    "    scheduler = LearningRateScheduler(\n",
    "        initial_lr=0.1,\n",
    "        schedule_type=schedule_type\n",
    "    )\n",
    "    \n",
    "    lrs = []\n",
    "    for i in range(100):\n",
    "        lrs.append(scheduler.get_lr())\n",
    "        scheduler.step()\n",
    "    \n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(lrs, linewidth=2, color='blue')\n",
    "    ax.set_title(f'{schedule_type.capitalize()} Learning Rate Decay')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Tips for Choosing Learning Rate\n",
    "\n",
    "1. **Start with default:** 0.01 is often reasonable starting point\n",
    "2. **Check loss plot:** Should decrease smoothly, not oscillate or increase\n",
    "3. **Scale with data:** Normalized data typically works with LR ~ 0.01-0.1\n",
    "4. **Use schedules:** Start high, decay over time often works well\n",
    "5. **Grid search:** Test multiple values: [0.001, 0.01, 0.1, 1.0]\n",
    "6. **Monitor convergence:** If not converging, reduce LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical learning rate selection\n",
    "learning_rates_to_test = [0.001, 0.01, 0.1, 1.0]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates_to_test:\n",
    "    model = SGDClassifier(\n",
    "        eta0=lr,\n",
    "        learning_rate='constant',\n",
    "        max_iter=50,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    accuracy = model.score(X, y)\n",
    "    results[lr] = accuracy\n",
    "    \n",
    "    print(f\"Learning Rate {lr:.4f}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Choose best learning rate\n",
    "best_lr = max(results, key=results.get)\n",
    "print(f\"\\nBest learning rate: {best_lr} with accuracy {results[best_lr]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}