{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Out-of-Core Learning\n",
    "\n",
    "### Definition\n",
    "**Out-of-Core Learning** (also called **External Memory Learning**) is the technique of training machine learning models on datasets that **cannot fit entirely into RAM**. Data is processed in chunks from disk storage.\n",
    "\n",
    "### The Problem It Solves\n",
    "\n",
    "Modern datasets are HUGE:\n",
    "- Netflix: billions of movies ratings\n",
    "- Google: terabytes of search queries daily\n",
    "- Financial institutions: petabytes of transaction data\n",
    "- Scientific simulations: multi-gigabyte datasets\n",
    "\n",
    "These cannot fit in standard computer memory!\n",
    "\n",
    "```\n",
    "Without Out-of-Core Learning:\n",
    "\u274c Cannot load 1TB dataset into 16GB RAM\n",
    "\u274c Must use expensive hardware or give up\n",
    "\n",
    "With Out-of-Core Learning:\n",
    "\u2705 Process data in manageable chunks\n",
    "\u2705 Use standard hardware\n",
    "\u2705 Train on massive datasets economically\n",
    "```\n",
    "\n",
    "### How Out-of-Core Learning Works\n",
    "\n",
    "#### Approach 1: Streaming Data Access\n",
    "```\n",
    "Load chunk 1 \u2192 Process \u2192 Update model \u2192 Discard chunk 1\n",
    "Load chunk 2 \u2192 Process \u2192 Update model \u2192 Discard chunk 2\n",
    "Load chunk 3 \u2192 Process \u2192 Update model \u2192 Discard chunk 3\n",
    "...\n",
    "Final model trained on ALL data without loading all at once\n",
    "```\n",
    "\n",
    "#### Approach 2: Incremental Learning\n",
    "```\n",
    "Model state (parameters) stays in memory\n",
    "Only current batch loaded from disk\n",
    "Update model with batch\n",
    "Discard batch, load next batch\n",
    "```\n",
    "\n",
    "### Python Examples: Out-of-Core Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "\n",
    "# Simulate huge CSV file (normally too large to load)\n",
    "# In real world, this might be 100GB+ file\n",
    "\n",
    "class HugeDataStreamSimulator:\n",
    "    \"\"\"Simulates reading huge CSV file in chunks\"\"\"\n",
    "    \n",
    "    def __init__(self, filename, chunksize=1000):\n",
    "        self.filename = filename\n",
    "        self.chunksize = chunksize\n",
    "    \n",
    "    def read_chunks(self):\n",
    "        \"\"\"Generator that yields chunks of data without loading entire file\"\"\"\n",
    "        # In real scenario: pd.read_csv(filename, chunksize=self.chunksize)\n",
    "        total_rows = 100_000\n",
    "        for i in range(0, total_rows, self.chunksize):\n",
    "            # Simulate reading chunk from disk\n",
    "            X_chunk = np.random.rand(min(self.chunksize, total_rows - i), 50)\n",
    "            y_chunk = np.random.randint(0, 2, min(self.chunksize, total_rows - i))\n",
    "            yield X_chunk, y_chunk\n",
    "\n",
    "# Method 1: Out-of-Core Learning with SGDClassifier\n",
    "print(\"Method 1: Out-of-Core Learning with Incremental Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = SGDClassifier(loss='log', random_state=42, warm_start=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_stream = HugeDataStreamSimulator(filename='huge_data.csv', chunksize=5000)\n",
    "\n",
    "samples_processed = 0\n",
    "batches_processed = 0\n",
    "\n",
    "for X_chunk, y_chunk in data_stream.read_chunks():\n",
    "    # Update statistics for scaling\n",
    "    if batches_processed == 0:\n",
    "        scaler.fit(X_chunk)\n",
    "    \n",
    "    X_scaled = scaler.transform(X_chunk)\n",
    "    \n",
    "    # Partial fit: update model with current chunk only\n",
    "    if batches_processed == 0:\n",
    "        model.partial_fit(X_scaled, y_chunk, classes=[0, 1])\n",
    "    else:\n",
    "        model.partial_fit(X_scaled, y_chunk)\n",
    "    \n",
    "    samples_processed += len(X_chunk)\n",
    "    batches_processed += 1\n",
    "    \n",
    "    if batches_processed % 5 == 0:\n",
    "        accuracy = model.score(X_scaled, y_chunk)\n",
    "        print(f\"Processed {samples_processed:,} samples ({batches_processed} batches), \"\n",
    "              f\"Current accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal samples processed: {samples_processed:,}\")\n",
    "print(f\"Total batches: {batches_processed}\")\n",
    "print(f\"Memory usage: Only one batch (~{5000 * 50 * 8 / (1024**2):.1f} MB) at a time\")\n",
    "\n",
    "# Method 2: Using Pandas read_csv with chunking (Real-world approach)\n",
    "print(\"\\n\\nMethod 2: Real-world CSV Chunking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sample CSV file\n",
    "sample_df = pd.DataFrame({\n",
    "    'feature_1': np.random.rand(10000),\n",
    "    'feature_2': np.random.rand(10000),\n",
    "    'feature_3': np.random.rand(10000),\n",
    "    'target': np.random.randint(0, 2, 10000)\n",
    "})\n",
    "sample_df.to_csv('large_sample.csv', index=False)\n",
    "\n",
    "# Read and train on chunks\n",
    "model2 = SGDClassifier(loss='log', random_state=42, warm_start=False)\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_count = 0\n",
    "\n",
    "for chunk in pd.read_csv('large_sample.csv', chunksize=chunk_size):\n",
    "    X = chunk.drop('target', axis=1).values\n",
    "    y = chunk['target'].values\n",
    "    \n",
    "    if chunk_count == 0:\n",
    "        model2.partial_fit(X, y, classes=[0, 1])\n",
    "    else:\n",
    "        model2.partial_fit(X, y)\n",
    "    \n",
    "    chunk_count += 1\n",
    "\n",
    "print(f\"Trained on {chunk_count} chunks from CSV file\")\n",
    "print(f\"Final accuracy: {model2.score(X, y):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for Out-of-Core Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apache Spark: Distributed out-of-core learning\n",
    "# pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OutOfCore\").getOrCreate()\n",
    "\n",
    "# Read huge CSV into Spark DataFrame (handles out-of-core automatically)\n",
    "df = spark.read.csv(\"huge_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Train on data that might be larger than single machine RAM\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01)\n",
    "model = lr.fit(df)\n",
    "\n",
    "predictions = model.transform(df)\n",
    "predictions.select(\"prediction\", \"label\").show()\n",
    "\n",
    "# 2. Dask: Parallel pandas for out-of-core data\n",
    "# pip install dask[dataframe]\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.array import from_delayed\n",
    "import dask.array as da\n",
    "\n",
    "# Load huge CSV with Dask (lazy evaluation, doesn't load all at once)\n",
    "ddf = dd.read_csv(\"huge_data.csv\")\n",
    "\n",
    "# Compute statistics without loading all data\n",
    "mean_values = ddf.mean().compute()\n",
    "print(f\"Mean of columns: {mean_values}\")\n",
    "\n",
    "# 3. Scikit-learn with partial_fit\n",
    "# Already demonstrated above\n",
    "\n",
    "# 4. TensorFlow with tf.data API\n",
    "# pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_dataset(features, labels, batch_size=1000):\n",
    "    \"\"\"Create dataset that yields batches for out-of-core training\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# Train model on huge dataset\n",
    "# model.fit(dataset)  # Automatically handles batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Out-of-Core Learning\n",
    "\n",
    "#### 1. **Handles Unlimited Data Size** \ud83d\udcca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can train on terabytes even with gigabytes of RAM\n",
    "# Example: 1TB dataset with 16GB RAM\n",
    "\n",
    "# Process 1TB dataset:\n",
    "# - Read 100MB chunks\n",
    "# - Process 100MB chunk\n",
    "# - Update model\n",
    "# - Total memory needed: ~200MB (model + batch)\n",
    "# - Train on 1TB with 16GB RAM easily!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Cost-Effective** \ud83d\udcb0\n",
    "- No need for expensive supercomputers\n",
    "- Regular hardware sufficient\n",
    "- Can process on edge devices\n",
    "- Reduces infrastructure costs\n",
    "\n",
    "#### 3. **Scalable** \ud83d\udcc8\n",
    "- Easily scales to growing datasets\n",
    "- No need to rebuild system for larger data\n",
    "- Seamlessly handle unlimited data streams\n",
    "\n",
    "#### 4. **Efficient** \u26a1\n",
    "- Memory usage remains constant regardless of dataset size\n",
    "- Computation spreads over time\n",
    "- Can prioritize resources\n",
    "\n",
    "### Disadvantages of Out-of-Core Learning\n",
    "\n",
    "#### 1. **Slower Training** \ud83d\udc22\n",
    "- I/O operations (disk read) are slow\n",
    "- Multiple passes through data needed\n",
    "- Slower than in-memory training\n",
    "- Disk bandwidth is limiting factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: In-Core vs Out-of-Core\n",
    "import time\n",
    "\n",
    "# In-Core: Load all data at once\n",
    "X_full = np.random.rand(100_000, 100)\n",
    "y_full = np.random.randint(0, 2, 100_000)\n",
    "\n",
    "start = time.time()\n",
    "model_in_core = SGDClassifier(max_iter=10)\n",
    "model_in_core.fit(X_full, y_full)\n",
    "in_core_time = time.time() - start\n",
    "print(f\"In-Core training: {in_core_time:.2f} seconds\")\n",
    "\n",
    "# Out-of-Core: Process in chunks from disk\n",
    "model_out_core = SGDClassifier(max_iter=1, warm_start=False)\n",
    "start = time.time()\n",
    "\n",
    "for i in range(0, len(X_full), 5000):\n",
    "    X_chunk = X_full[i:i+5000]\n",
    "    y_chunk = y_full[i:i+5000]\n",
    "    \n",
    "    if i == 0:\n",
    "        model_out_core.partial_fit(X_chunk, y_chunk, classes=[0, 1])\n",
    "    else:\n",
    "        model_out_core.partial_fit(X_chunk, y_chunk)\n",
    "\n",
    "out_core_time = time.time() - start\n",
    "print(f\"Out-of-Core training: {out_core_time:.2f} seconds\")\n",
    "print(f\"Overhead: {out_core_time/in_core_time:.2f}x slower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Implementation Complexity** \ud83d\udd27\n",
    "- More code to write\n",
    "- Error handling more involved\n",
    "- Debugging harder (data constantly changing)\n",
    "- State management required\n",
    "\n",
    "#### 3. **Limited Model Options**\n",
    "- Not all algorithms support incremental learning\n",
    "- Deep learning models harder to implement out-of-core\n",
    "- Some models require seeing all data\n",
    "\n",
    "### When to Use Out-of-Core Learning\n",
    "\n",
    "\u2705 **Use Out-of-Core when:**\n",
    "- Dataset too large for available RAM\n",
    "- Streaming data constantly arriving\n",
    "- Cannot afford powerful hardware\n",
    "- Training on edge devices\n",
    "- Processing real-time data streams\n",
    "\n",
    "\u274c **Don't use when:**\n",
    "- Data fits comfortably in RAM\n",
    "- Speed is critical\n",
    "- Need maximum model accuracy\n",
    "- Using complex deep learning models\n",
    "\n",
    "### Real-World Example: Processing Million-Row Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import gc\n",
    "\n",
    "class OutOfCoreMLPipeline:\n",
    "    \"\"\"Complete out-of-core ML pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='classification'):\n",
    "        self.model = SGDClassifier(loss='log', n_jobs=-1) if \\\n",
    "                    model_type == 'classification' else SGDRegressor()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit_on_csv(self, csv_filepath, chunksize=10000, \n",
    "                   target_column='target', categorical_columns=None):\n",
    "        \"\"\"Train model on huge CSV file chunk by chunk\"\"\"\n",
    "        \n",
    "        categorical_columns = categorical_columns or []\n",
    "        chunks_processed = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(csv_filepath, chunksize=chunksize):\n",
    "            # Separate features and target\n",
    "            y = chunk[target_column].values\n",
    "            X = chunk.drop(target_column, axis=1)\n",
    "            \n",
    "            # Handle categorical features\n",
    "            for col in categorical_columns:\n",
    "                if col in X.columns:\n",
    "                    X[col] = self.label_encoder.fit_transform(X[col])\n",
    "            \n",
    "            # Convert to numpy\n",
    "            X = X.values\n",
    "            \n",
    "            # Scale features\n",
    "            if not self.is_fitted:\n",
    "                self.scaler.fit(X)\n",
    "                self.feature_names = X.shape[1]\n",
    "            \n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            \n",
    "            # Update model\n",
    "            if not self.is_fitted:\n",
    "                self.model.partial_fit(X_scaled, y, classes=np.unique(y))\n",
    "                self.is_fitted = True\n",
    "            else:\n",
    "                self.model.partial_fit(X_scaled, y)\n",
    "            \n",
    "            chunks_processed += 1\n",
    "            if chunks_processed % 10 == 0:\n",
    "                acc = self.model.score(X_scaled, y)\n",
    "                print(f\"Processed {chunks_processed*chunksize:,} rows, \"\n",
    "                      f\"Current accuracy: {acc:.4f}\")\n",
    "            \n",
    "            # Free memory\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"\\nTotal chunks processed: {chunks_processed}\")\n",
    "        return self\n",
    "    \n",
    "    def predict_on_csv(self, csv_filepath, output_filepath, chunksize=10000):\n",
    "        \"\"\"Make predictions on huge CSV file\"\"\"\n",
    "        \n",
    "        predictions_list = []\n",
    "        \n",
    "        for chunk in pd.read_csv(csv_filepath, chunksize=chunksize):\n",
    "            X = chunk.values\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            preds = self.model.predict(X_scaled)\n",
    "            predictions_list.append(preds)\n",
    "            \n",
    "            gc.collect()\n",
    "        \n",
    "        # Save predictions\n",
    "        all_predictions = np.concatenate(predictions_list)\n",
    "        np.save(output_filepath, all_predictions)\n",
    "        print(f\"Predictions saved to {output_filepath}\")\n",
    "        return all_predictions\n",
    "\n",
    "# Usage\n",
    "pipeline = OutOfCoreMLPipeline()\n",
    "# pipeline.fit_on_csv('million_rows.csv', chunksize=50000, \n",
    "#                     target_column='label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}