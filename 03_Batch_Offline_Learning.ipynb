{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch (Offline) Learning\n",
    "\n",
    "### Definition\n",
    "**Batch Learning** (also called **Offline Learning**) trains a model on the **entire fixed dataset all at once**. The model is then deployed and doesn't update until it's explicitly retrained on a new batch of data.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "[Training Data] \u2192 [Model Training on Entire Dataset] \u2192 [Trained Model] \u2192 [Deployment]\n",
    "                         (one iteration)                                      \u2193\n",
    "                                                                         [Predictions]\n",
    "```\n",
    "\n",
    "The model sees all data simultaneously and learns parameters from the complete dataset before making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b634ab",
   "metadata": {},
   "source": [
    "### Characteristics\n",
    "\n",
    "1. **Complete Dataset Required:** Must have entire dataset available before training\n",
    "2. **Single Training Phase:** Train once on all data\n",
    "3. **Parameter Update:** Parameters finalized after seeing all data\n",
    "4. **Resource Intensive:** Requires significant computation, memory, and storage\n",
    "5. **Model Stability:** Once trained, model remains static until retraining\n",
    "6. **High Latency:** Long gap between data collection and model update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Batch Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046858c",
   "metadata": {},
   "source": [
    "#### 1. **High Accuracy**\n",
    "- Model sees all available data and patterns\n",
    "- Can perform deep analysis of entire dataset\n",
    "- Better understanding of data distribution\n",
    "- More stable parameter estimates\n",
    "\n",
    "```python\n",
    "# Example: Training on full dataset produces stable results\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Batch learning: train on entire dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)  # Trains on full training set\n",
    "\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Batch Learning Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "#### 2. **Stability**\n",
    "- Consistent results across multiple runs (with same random seed)\n",
    "- No fluctuations in model behavior\n",
    "- Predictable performance\n",
    "- Easier debugging and testing\n",
    "\n",
    "#### 3. **Simplicity in Implementation**\n",
    "- Straightforward training pipeline\n",
    "- Easier to understand and implement\n",
    "- Well-established tools and frameworks (scikit-learn, TensorFlow)\n",
    "- Less complex error handling\n",
    "\n",
    "#### 4. **Distributed Computing Advantage**\n",
    "- Can leverage parallel processing on clusters\n",
    "- Easier to distribute computation across multiple machines\n",
    "- Frameworks like Spark, Hadoop optimize batch processing\n",
    "- Cost-effective for large-scale training on cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda31f",
   "metadata": {},
   "source": [
    "### Disadvantages of Batch Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f410d3",
   "metadata": {},
   "source": [
    "#### 1. **Resource Intensive** \u26a0\ufe0f\n",
    "- **Memory:** Entire dataset must be loaded into RAM\n",
    "- **Computation:** Processing huge datasets takes considerable time\n",
    "- **Storage:** Need disk space for complete dataset\n",
    "- **Cost:** Cloud resources expensive for large-scale training\n",
    "\n",
    "```python\n",
    "# Example: Memory constraints with large datasets\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "# Check available memory\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "print(f\"Available Memory: {available_memory_gb:.2f} GB\")\n",
    "\n",
    "# Large dataset example\n",
    "large_array = np.random.rand(1_000_000, 1000)  # ~8GB\n",
    "print(f\"Array size: {large_array.nbytes / (1024**3):.2f} GB\")\n",
    "# This might crash if insufficient memory!\n",
    "```\n",
    "\n",
    "#### 2. **Slow Adaptation to New Data** \u274c\n",
    "- Cannot adapt quickly when new data arrives\n",
    "- Must complete full retraining to incorporate new information\n",
    "- Retraining takes days or weeks for large datasets\n",
    "- Old model serves predictions while new model trains\n",
    "\n",
    "```\n",
    "Day 1: Collect data \u2192 Train model (takes 5 days)\n",
    "Day 5: Deploy model\n",
    "Day 6: New data arrives (model doesn't adapt)\n",
    "       Market has changed, but old model still predicts based on old patterns\n",
    "Day 11: Retraining completes, deploy new model\n",
    "       But 5 days of outdated predictions already made!\n",
    "```\n",
    "\n",
    "#### 3. **High Latency**\n",
    "- Long delay between data arrival and model update\n",
    "- Not suitable for real-time systems requiring quick adaptation\n",
    "- Cannot handle concept drift (when data patterns change over time)\n",
    "\n",
    "Real-world concept drift example:\n",
    "```\n",
    "E-commerce fraud detection:\n",
    "- Model trained on 2020 fraud patterns\n",
    "- By 2024, fraudsters use different tactics\n",
    "- Model becomes ineffective, but retraining takes 2 weeks\n",
    "- During those 2 weeks, fraudulent transactions slip through\n",
    "```\n",
    "\n",
    "#### 4. **Inflexibility**\n",
    "- Once trained, model is \"frozen\"\n",
    "- No way to improve performance incrementally\n",
    "- If error found in data, entire retraining needed\n",
    "- Cannot respond to rapid business changes\n",
    "\n",
    "#### 5. **Data Redundancy Issues**\n",
    "- Large datasets often contain redundant information\n",
    "- Wasted computation on similar samples\n",
    "- Full retraining even if only small portion of data changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94e7d1",
   "metadata": {},
   "source": [
    "### Disadvantages: Python Examples\n",
    "\n",
    "```python\n",
    "# Problem 1: Time-Consuming Training\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate large dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=100_000,  # 100k samples\n",
    "    n_features=100,\n",
    "    n_informative=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "svm = SVC(kernel='rbf', C=1.0)\n",
    "svm.fit(X, y)  # This will take significant time\n",
    "elapsed = time.time() - start\n",
    "print(f\"Training time for 100k samples: {elapsed:.2f} seconds\")\n",
    "\n",
    "# Problem 2: Cannot Adapt to New Data\n",
    "import numpy as np\n",
    "\n",
    "# Initial training data\n",
    "X_initial = np.random.rand(10000, 20)\n",
    "y_initial = np.random.randint(0, 2, 10000)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_initial, y_initial)\n",
    "print(f\"Model trained on {len(X_initial)} samples\")\n",
    "\n",
    "# New data arrives\n",
    "X_new = np.random.rand(5000, 20)  # 5000 new samples\n",
    "# Problem: Cannot update model with new data\n",
    "# Must retrain from scratch!\n",
    "model.fit(np.vstack([X_initial, X_new]), \n",
    "          np.hstack([y_initial, np.random.randint(0, 2, 5000)]))\n",
    "print(\"Had to retrain entire model with all data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f034a28",
   "metadata": {},
   "source": [
    "### Real-World Applications Where Batch Learning Works Well\n",
    "\n",
    "1. **Recommendation Systems (Netflix)**\n",
    "   - Train models daily on all user ratings\n",
    "   - Deploy model for entire day\n",
    "   - Update next day with new ratings\n",
    "\n",
    "2. **Fraud Detection (Banks)**\n",
    "   - Train models weekly on transaction history\n",
    "   - Detect anomalies throughout the week\n",
    "   - Retrain next week with new fraud patterns\n",
    "\n",
    "3. **Price Prediction (Real Estate)**\n",
    "   - Train model monthly on all available listings\n",
    "   - Use for predictions throughout month\n",
    "   - Retrain when significant market data accumulated\n",
    "\n",
    "4. **Medical Imaging (Hospital Systems)**\n",
    "   - Train models on large collected datasets\n",
    "   - Deploy for diagnosis assistance\n",
    "   - Retrain periodically when new cases collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf527b",
   "metadata": {},
   "source": [
    "### Tools for Batch Learning\n",
    "\n",
    "```python\n",
    "# Scikit-learn: Batch learning framework\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create batch learning pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', GradientBoostingClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "# Train on entire batch\n",
    "pipeline.fit(X_train, y_train)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "\n",
    "# Apache Spark: Distributed batch learning\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BatchML\").getOrCreate()\n",
    "spark_df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=100, featuresCol=\"features\")\n",
    "# Trains on all data in spark cluster\n",
    "model = rf.fit(spark_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b0a598",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3c05471",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b610a045",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2a3099",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af6254a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f01e580a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caab47d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1483c5d",
   "metadata": {},
   "source": [
    "* Challenges in Machine Learning\n",
    "* Problems in Machine Learning\n",
    "    - Data Collection - API or WebScrapping\n",
    "    - Insufficient Data/ Labelled Data\n",
    "    - Non representative data - sampling noise, sampling bias\n",
    "    - poor quality data\n",
    "    - irrelevant features (Garbage IN --> Garbage out)\n",
    "    - overfitting \n",
    "    - underfitting\n",
    "    - software integration\n",
    "    - Offline learning/ deplyoment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92311320",
   "metadata": {},
   "source": [
    "### Key Takeaway for Batch Learning\n",
    "Use batch learning when:\n",
    "- \u2705 You have complete, stable dataset\n",
    "- \u2705 Accurate predictions matter more than speed\n",
    "- \u2705 Data patterns change slowly\n",
    "- \u2705 Real-time adaptation not required\n",
    "- \u2705 Can afford computational resources for full retraining\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}