{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3424d2c",
   "metadata": {},
   "source": [
    "## 2. Types of Machine Learning\n",
    "\n",
    "### Overview\n",
    "Machine Learning is fundamentally categorized by **how algorithms learn** from data.\n",
    "\n",
    "### 2.1 **Supervised Learning**\n",
    "- **Core Concept:** Learn from labeled examples to predict outcomes\n",
    "- **Training Process:** Model learns mapping from features \u2192 labels\n",
    "- **Data Required:** Large amount of labeled training data\n",
    "- **Feedback:** Explicit target values guide learning\n",
    "\n",
    "#### Classification Tasks\n",
    "- **Goal:** Predict discrete categories/classes\n",
    "- **Output:** Class label (e.g., SPAM/NOT_SPAM, Cat/Dog/Bird)\n",
    "- **Examples:**\n",
    "  - Email spam detection (Spam vs Not Spam)\n",
    "  - Medical diagnosis (Disease vs Healthy)\n",
    "  - Image classification (identify objects)\n",
    "  - Sentiment analysis (Positive/Negative)\n",
    "- **Algorithms:** Logistic Regression, Decision Trees, SVM, Neural Networks, Random Forest\n",
    "- **Metrics:** Accuracy, Precision, Recall, F1-Score, Confusion Matrix\n",
    "\n",
    "#### Regression Tasks\n",
    "- **Goal:** Predict continuous numerical values\n",
    "- **Output:** Real number (e.g., price, temperature, salary)\n",
    "- **Examples:**\n",
    "  - House price prediction based on features (size, location, age)\n",
    "  - Stock price forecasting\n",
    "  - Temperature prediction\n",
    "  - Salary estimation\n",
    "- **Algorithms:** Linear Regression, Polynomial Regression, SVR, Neural Networks\n",
    "- **Metrics:** MSE (Mean Squared Error), RMSE, MAE (Mean Absolute Error), R\u00b2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee212b5",
   "metadata": {},
   "source": [
    "#### Real-World Examples:\n",
    "```python\n",
    "# Classification Example\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target  # Labeled: 0, 1, 2 (three iris species)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(f\"Classification Accuracy: {accuracy_score(y_test, predictions):.2%}\")\n",
    "\n",
    "# Regression Example\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample: predict house prices\n",
    "X_houses = np.array([[1000], [1500], [2000], [2500]])  # sq ft\n",
    "y_prices = np.array([200000, 300000, 400000, 500000])  # price in $\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_houses, y_prices)\n",
    "predicted_price = reg.predict([[1800]])\n",
    "print(f\"Predicted price for 1800 sqft: ${predicted_price[0]:,.0f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98288b6e",
   "metadata": {},
   "source": [
    "### 2.2 **Unsupervised Learning**\n",
    "- **Core Concept:** Discover hidden patterns without labels\n",
    "- **Training Process:** Find structure and relationships in unlabeled data\n",
    "- **Data Required:** Unlabeled data (abundant, cheaper)\n",
    "- **No Explicit Feedback:** Model must find patterns independently\n",
    "\n",
    "#### Clustering\n",
    "- **Goal:** Group similar data points together\n",
    "- **Output:** Group assignments (clusters)\n",
    "- **Examples:**\n",
    "  - Customer segmentation (identify customer groups for targeted marketing)\n",
    "  - Gene clustering (group genes with similar expressions)\n",
    "  - Document clustering (group similar documents/news articles)\n",
    "  - Image clustering (group similar images)\n",
    "- **Algorithms:** K-Means, Hierarchical Clustering, DBSCAN, Gaussian Mixture Models\n",
    "- **Distance Metrics:** Euclidean distance, Cosine similarity, Manhattan distance\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "- **Goal:** Reduce number of features while preserving important information\n",
    "- **Benefits:** Visualization, noise reduction, computational efficiency\n",
    "- **Examples:**\n",
    "  - PCA (Principal Component Analysis): reduce 10 features to 3 without losing much info\n",
    "  - t-SNE: visualize high-dimensional data in 2D\n",
    "  - Feature selection: identify most important features\n",
    "- **Algorithms:** PCA, t-SNE, Autoencoders, Feature Selection\n",
    "\n",
    "#### Association Rules\n",
    "- **Goal:** Find relationships and patterns between variables\n",
    "- **Examples:**\n",
    "  - Market basket analysis (customers who buy X also buy Y)\n",
    "  - Recommendation systems\n",
    "- **Output:** Rules like \"If customer buys milk and bread, they likely buy butter\"\n",
    "- **Algorithms:** Apriori, Eclat, FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3f293",
   "metadata": {},
   "source": [
    "```python\n",
    "# Unsupervised: Clustering Example\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data\n",
    "X, _ = make_blobs(n_samples=100, centers=3, random_state=42)\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "print(f\"Cluster centers:\\n{kmeans.cluster_centers_}\")\n",
    "print(f\"Sample point belongs to cluster: {clusters[0]}\")\n",
    "\n",
    "# Dimensionality Reduction Example\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Reduced shape: {X_reduced.shape}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7659bc",
   "metadata": {},
   "source": [
    "### 2.3 **Reinforcement Learning**\n",
    "- **Core Concept:** Learn through trial and error with rewards and penalties\n",
    "- **Training Process:** Agent takes actions, receives rewards/penalties, learns optimal behavior\n",
    "- **Key Component:** Environment, Agent, State, Action, Reward\n",
    "- **No Labeled Data:** Learns from interaction feedback\n",
    "- **Goal:** Maximize cumulative reward over time\n",
    "\n",
    "#### How It Works:\n",
    "1. **Agent** observes the **State** of environment\n",
    "2. Agent takes an **Action**\n",
    "3. Environment provides **Reward** (positive/negative)\n",
    "4. Agent transitions to new **State**\n",
    "5. Repeat: Learn which actions lead to maximum rewards\n",
    "\n",
    "#### Real-World Applications:\n",
    "- **Autonomous Vehicles:** Learn optimal driving behavior (reward: safe arrival, penalty: collision)\n",
    "- **Game AI:** AlphaGo learned to beat humans at Go through self-play\n",
    "- **Robotics:** Robot learns to grasp objects through trial and error\n",
    "- **Trading Systems:** Learn optimal trading strategies (reward: profit, penalty: loss)\n",
    "- **Recommendation Systems:** Learn which recommendations users prefer\n",
    "\n",
    "#### Key Algorithms:\n",
    "- **Q-Learning:** Learn Q-values (expected future reward for each action)\n",
    "- **SARSA:** On-policy learning (learns from actual actions taken)\n",
    "- **Deep Q-Networks (DQN):** Combine Q-Learning with neural networks\n",
    "- **Policy Gradient Methods:** Directly learn the policy (mapping state \u2192 action)\n",
    "- **Actor-Critic Methods:** Combination of policy and value learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8760cf",
   "metadata": {},
   "source": [
    "```python\n",
    "# Simplified Reinforcement Learning Example: Grid World\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_size=5):\n",
    "        self.grid_size = grid_size\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [grid_size-1, grid_size-1]\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Actions: 0=up, 1=down, 2=left, 3=right\n",
    "        Returns: new_state, reward\n",
    "        \"\"\"\n",
    "        new_pos = self.agent_pos.copy()\n",
    "        \n",
    "        if action == 0:  # up\n",
    "            new_pos[0] = max(0, new_pos[0] - 1)\n",
    "        elif action == 1:  # down\n",
    "            new_pos[1] = min(self.grid_size-1, new_pos[1] + 1)\n",
    "        elif action == 2:  # left\n",
    "            new_pos[0] = max(0, new_pos[0] - 1)\n",
    "        elif action == 3:  # right\n",
    "            new_pos[0] = min(self.grid_size-1, new_pos[0] + 1)\n",
    "        \n",
    "        self.agent_pos = new_pos\n",
    "        \n",
    "        # Reward: +10 for reaching goal, -1 for each step (encourage efficiency)\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return tuple(self.agent_pos), reward\n",
    "\n",
    "# Simple Q-Learning example\n",
    "env = GridWorld()\n",
    "Q = {}  # Q-table: maps (state, action) \u2192 expected future reward\n",
    "learning_rate = 0.1\n",
    "discount = 0.9\n",
    "\n",
    "# Training loop\n",
    "for episode in range(100):\n",
    "    state = (0, 0)\n",
    "    for step in range(20):\n",
    "        # Epsilon-greedy: explore randomly sometimes, exploit best action other times\n",
    "        if np.random.random() < 0.1:  # 10% explore\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:  # 90% exploit\n",
    "            q_values = [Q.get((state, a), 0) for a in range(4)]\n",
    "            action = np.argmax(q_values)\n",
    "        \n",
    "        new_state, reward = env.step(action)\n",
    "        \n",
    "        # Q-Learning update\n",
    "        max_next_q = max([Q.get((new_state, a), 0) for a in range(4)])\n",
    "        old_q = Q.get((state, action), 0)\n",
    "        Q[(state, action)] = old_q + learning_rate * (\n",
    "            reward + discount * max_next_q - old_q\n",
    "        )\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "print(f\"Learned Q-values: {len(Q)} state-action pairs\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cfefa",
   "metadata": {},
   "source": [
    "### Comparison Table\n",
    "\n",
    "| Aspect | Supervised | Unsupervised | Reinforcement |\n",
    "|--------|-----------|-------------|---------------|\n",
    "| **Data Type** | Labeled | Unlabeled | Interactive Environment |\n",
    "| **Training Goal** | Predict output | Find patterns | Maximize rewards |\n",
    "| **Feedback** | Explicit labels | Self-discovery | Trial and error |\n",
    "| **Examples** | Classification, Regression | Clustering, Dimensionality reduction | Game playing, Robotics |\n",
    "| **Data Requirement** | Large labeled dataset | Abundant unlabeled data | Simulation environment |\n",
    "| **Complexity** | Moderate | High | Very High |\n",
    "| **Interpretability** | Generally high | Medium | Low (black box) |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}