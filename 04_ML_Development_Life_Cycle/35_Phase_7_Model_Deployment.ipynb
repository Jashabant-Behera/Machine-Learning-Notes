{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Model Deployment\n",
    "\n",
    "### What is Deployment?\n",
    "\n",
    "Deployment means taking a trained model and putting it into production so it can make real predictions on new data.\n",
    "\n",
    "### Deployment Architectures\n",
    "\n",
    "**1. Batch Prediction (Offline)**\n",
    "\n",
    "Used when: Predictions needed daily/weekly, not in real-time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'models/churn_model.joblib')\n",
    "\n",
    "# In production, load and make predictions on new data\n",
    "def batch_predict(new_data_path):\n",
    "    \"\"\"\n",
    "    Load new data and make predictions\n",
    "    \"\"\"\n",
    "    model = joblib.load('models/churn_model.joblib')\n",
    "    new_data = pd.read_csv(new_data_path)\n",
    "    \n",
    "    # Preprocess\n",
    "    new_data_processed = preprocessor.transform(new_data)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict_proba(new_data_processed)[:, 1]\n",
    "    \n",
    "    # Save results\n",
    "    results = pd.DataFrame({\n",
    "        'customer_id': new_data['customer_id'],\n",
    "        'churn_probability': predictions,\n",
    "        'prediction_date': pd.Timestamp.now()\n",
    "    })\n",
    "    \n",
    "    results.to_csv('outputs/churn_predictions.csv', index=False)\n",
    "    return results\n",
    "\n",
    "# Schedule with Airflow\n",
    "# Runs daily at 2 AM, processes all customers, outputs results to database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Real-Time API (Online)**\n",
    "\n",
    "Used when: Predictions needed immediately (fraud detection, recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using FastAPI (modern Python web framework)\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "app = FastAPI()\n",
    "model = joblib.load('models/churn_model.joblib')\n",
    "\n",
    "class PredictionInput(BaseModel):\n",
    "    customer_id: int\n",
    "    age: int\n",
    "    total_spent: float\n",
    "    transaction_count: int\n",
    "    account_age_days: int\n",
    "    account_type: str\n",
    "    country: str\n",
    "\n",
    "@app.post(\"/predict-churn\")\n",
    "async def predict_churn(input_data: PredictionInput):\n",
    "    \"\"\"\n",
    "    Endpoint to predict churn probability\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X = pd.DataFrame([input_data.dict()])\n",
    "    \n",
    "    # Preprocess\n",
    "    X_processed = preprocessor.transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    churn_prob = float(model.predict_proba(X_processed)[0, 1])\n",
    "    prediction = int(model.predict(X_processed)[0])\n",
    "    \n",
    "    return {\n",
    "        'customer_id': input_data.customer_id,\n",
    "        'churn_probability': churn_prob,\n",
    "        'prediction': 'Will Churn' if prediction == 1 else 'Will Not Churn',\n",
    "        'recommendation': 'Send retention offer' if churn_prob > 0.7 else 'No action needed'\n",
    "    }\n",
    "\n",
    "# Deploy with Docker\n",
    "# curl -X POST \"http://localhost:8000/predict-churn\" \\\n",
    "#      -H \"Content-Type: application/json\" \\\n",
    "#      -d '{\"customer_id\": 123, \"age\": 45, \"total_spent\": 5000.0, ...}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Containerization with Docker**\n",
    "\n",
    "Ensures model runs identically everywhere (laptop, server, cloud)\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy model and code\n",
    "COPY churn_model.joblib .\n",
    "COPY api.py .\n",
    "COPY preprocessor.joblib .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run API\n",
    "CMD [\"uvicorn\", \"api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Build and run\n",
    "docker build -t churn-model .\n",
    "docker run -p 8000:8000 churn-model\n",
    "```\n",
    "\n",
    "**4. Model Serving Platforms**\n",
    "\n",
    "For production-scale serving:\n",
    "\n",
    "```\n",
    "Option 1: TensorFlow Serving\n",
    "- Optimized for deep learning\n",
    "- High throughput, low latency\n",
    "- Used by Google internally\n",
    "\n",
    "Option 2: Seldon\n",
    "- Kubernetes-native model serving\n",
    "- Supports any ML framework\n",
    "- Includes A/B testing, canary deployments\n",
    "\n",
    "Option 3: KServe\n",
    "- Built on Kubernetes\n",
    "- Multi-framework support\n",
    "- Automatic scaling\n",
    "\n",
    "Option 4: Cloud Platforms\n",
    "- AWS SageMaker\n",
    "- Google Vertex AI\n",
    "- Azure Machine Learning\n",
    "- Simple one-click deployment, scales automatically\n",
    "```\n",
    "\n",
    "### Deployment Checklist\n",
    "\n",
    "```\n",
    "Pre-Deployment:\n",
    "\n",
    "[\u2713] Model trained and validated\n",
    "[\u2713] Preprocessing pipeline saved with model\n",
    "[\u2713] Performance metrics documented\n",
    "[\u2713] API endpoints defined\n",
    "[\u2713] Input validation implemented\n",
    "[\u2713] Error handling added\n",
    "[\u2713] Logging configured\n",
    "[\u2713] Security reviewed (authentication, rate limiting)\n",
    "\n",
    "Post-Deployment:\n",
    "\n",
    "[\u2713] Monitor prediction latency\n",
    "[\u2713] Monitor error rates\n",
    "[\u2713] Track model performance (prediction accuracy)\n",
    "[\u2713] Monitor data drift (input distribution changes)\n",
    "[\u2713] Set up alerting for failures\n",
    "[\u2713] Document API usage\n",
    "[\u2713] Plan for model updates\n",
    "```\n",
    "\n",
    "### Tools Used in Deployment\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| Docker | Containerization |\n",
    "| FastAPI | REST API framework |\n",
    "| Flask | Lightweight API framework |\n",
    "| TensorFlow Serving | ML model serving |\n",
    "| Seldon | Model serving on Kubernetes |\n",
    "| AWS SageMaker | Cloud model deployment |\n",
    "| Kubernetes | Container orchestration |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}