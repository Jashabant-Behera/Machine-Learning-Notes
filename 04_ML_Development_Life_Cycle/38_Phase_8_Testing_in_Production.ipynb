{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 8: Testing in Production\n",
    "\n",
    "### Types of Testing\n",
    "\n",
    "**1. Data Validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great Expectations: Validate data quality in production\n",
    "from great_expectations.dataset import PandasDataset\n",
    "\n",
    "# Define expectations\n",
    "expectations = PandasDataset(new_data).expect_table_row_count_to_be_between(\n",
    "    min_value=1000,\n",
    "    max_value=100000\n",
    ")\n",
    "\n",
    "# Check age is in valid range\n",
    "expectations.expect_column_values_to_be_between(\n",
    "    column='age',\n",
    "    min_value=18,\n",
    "    max_value=120\n",
    ")\n",
    "\n",
    "# Check no missing values\n",
    "expectations.expect_column_values_to_not_be_null(column='customer_id')\n",
    "\n",
    "# Get validation report\n",
    "validation_result = expectations.validate()\n",
    "print(validation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Model Performance Monitoring**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor prediction distribution\n",
    "# If distribution changes drastically, model might be broken\n",
    "\n",
    "def monitor_predictions(predictions):\n",
    "    \"\"\"\n",
    "    Track prediction statistics over time\n",
    "    \"\"\"\n",
    "    monitoring_stats = {\n",
    "        'mean_churn_prob': predictions.mean(),\n",
    "        'std_churn_prob': predictions.std(),\n",
    "        'min': predictions.min(),\n",
    "        'max': predictions.max(),\n",
    "        'high_risk_count': (predictions > 0.7).sum(),\n",
    "        'low_risk_count': (predictions < 0.3).sum(),\n",
    "        'timestamp': pd.Timestamp.now()\n",
    "    }\n",
    "    \n",
    "    # Save to time series database\n",
    "    # Alert if mean_churn_prob suddenly doubles\n",
    "    return monitoring_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. A/B Testing (Champion vs Challenger)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split traffic between old model (champion) and new model (challenger)\n",
    "# Compare performance metrics\n",
    "\n",
    "def run_ab_test(data, champion_model, challenger_model, split_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Run A/B test between two models\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    split_point = int(n * split_ratio)\n",
    "    \n",
    "    # Champion (old model)\n",
    "    champion_preds = champion_model.predict(data[:split_point])\n",
    "    \n",
    "    # Challenger (new model)\n",
    "    challenger_preds = challenger_model.predict(data[split_point:])\n",
    "    \n",
    "    # Compare metrics\n",
    "    champion_auc = roc_auc_score(y_true[:split_point], champion_preds)\n",
    "    challenger_auc = roc_auc_score(y_true[split_point:], challenger_preds)\n",
    "    \n",
    "    print(f\"Champion AUC: {champion_auc:.4f}\")\n",
    "    print(f\"Challenger AUC: {challenger_auc:.4f}\")\n",
    "    \n",
    "    if challenger_auc > champion_auc + 0.01:  # 1% improvement threshold\n",
    "        print(\"Challenger wins! Promote to production.\")\n",
    "        return 'challenger'\n",
    "    else:\n",
    "        print(\"Champion still better. Keep current model.\")\n",
    "        return 'champion'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Monitoring Data Drift**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data drift: Input distribution changes over time\n",
    "# Example: Sudden increase in old customers, model was trained on younger customers\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def detect_data_drift(X_train, X_new):\n",
    "    \"\"\"\n",
    "    Detect if new data distribution differs from training data\n",
    "    \"\"\"\n",
    "    for column in X_train.columns:\n",
    "        statistic, p_value = ks_2samp(X_train[column], X_new[column])\n",
    "        \n",
    "        if p_value < 0.05:  # Statistically significant difference\n",
    "            print(f\"Data drift detected in column: {column}\")\n",
    "            print(f\"p-value: {p_value}\")\n",
    "            # Action: Retrain model on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used in Testing\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| Great Expectations | Data validation |\n",
    "| Evidently | Model monitoring |\n",
    "| Prometheus | Metrics collection |\n",
    "| Grafana | Monitoring dashboards |\n",
    "| MLflow | Model tracking |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}