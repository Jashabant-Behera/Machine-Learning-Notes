{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 9: Optimization & Continuous Improvement\n",
    "\n",
    "### Continuous Model Improvement\n",
    "\n",
    "**1. Retraining Schedule**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how often to retrain based on:\n",
    "# - How fast patterns change\n",
    "# - How much new data accumulates\n",
    "# - Computational resources\n",
    "\n",
    "# Example: Churn model retrained weekly\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "def retrain_model():\n",
    "    \"\"\"\n",
    "    Retrain model with latest data\n",
    "    \"\"\"\n",
    "    # Load new data (last 30 days)\n",
    "    new_data = load_recent_data(days=30)\n",
    "    \n",
    "    # Combine with historical data for stability\n",
    "    combined_data = pd.concat([historical_data, new_data])\n",
    "    \n",
    "    # Preprocess, train, validate\n",
    "    X = preprocessor.fit_transform(combined_data)\n",
    "    y = combined_data['churned']\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Train new model\n",
    "    new_model = RandomForestClassifier(n_estimators=100)\n",
    "    new_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Validate: Is new model better than current model?\n",
    "    new_auc = roc_auc_score(y_test, new_model.predict_proba(X_test)[:, 1])\n",
    "    current_auc = evaluate_current_model(X_test, y_test)\n",
    "    \n",
    "    if new_auc > current_auc + 0.01:  # Only promote if 1% improvement\n",
    "        promote_model(new_model)\n",
    "        print(f\"Model updated. New AUC: {new_auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"New model not better. Current AUC: {current_auc:.4f}, New: {new_auc:.4f}\")\n",
    "\n",
    "# Schedule\n",
    "schedule.every().friday.at(\"02:00\").do(retrain_model)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature Importance Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand which features drive predictions\n",
    "# Use for model debugging and business insights\n",
    "\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze which features matter most\n",
    "    \"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(importance_df)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Insights: If top 5 features explain 80% of importance,\n",
    "    # model is interpretable and stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Error Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When model makes mistakes, analyze why\n",
    "\n",
    "def analyze_errors(y_true, y_pred, y_proba, data):\n",
    "    \"\"\"\n",
    "    Analyze model errors for insights\n",
    "    \"\"\"\n",
    "    # False positives: Predicted churn but didn't\n",
    "    false_positives = (y_pred == 1) & (y_true == 0)\n",
    "    fp_data = data[false_positives]\n",
    "    \n",
    "    print(\"False Positives (high churn prediction but stayed):\")\n",
    "    print(fp_data[['age', 'total_spent', 'transaction_count']].describe())\n",
    "    \n",
    "    # False negatives: Predicted no churn but actually churned\n",
    "    false_negatives = (y_pred == 0) & (y_true == 1)\n",
    "    fn_data = data[false_negatives]\n",
    "    \n",
    "    print(\"\\nFalse Negatives (missed churners):\")\n",
    "    print(fn_data[['age', 'total_spent', 'transaction_count']].describe())\n",
    "    \n",
    "    # Insights:\n",
    "    # If FNs are old customers with high spend, might need segment-specific models\n",
    "    # If FPs are new customers, might have cold start problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Model Performance Degradation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor if model performance decreases over time\n",
    "\n",
    "def monitor_performance_degradation(dates, performance_scores):\n",
    "    \"\"\"\n",
    "    Track performance over time\n",
    "    \"\"\"\n",
    "    performance_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'auc': performance_scores\n",
    "    })\n",
    "    \n",
    "    # Calculate trend\n",
    "    performance_df['auc_rolling_avg'] = performance_df['auc'].rolling(7).mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(performance_df['date'], performance_df['auc'], label='Daily AUC', alpha=0.5)\n",
    "    plt.plot(performance_df['date'], performance_df['auc_rolling_avg'], \n",
    "             label='7-day Average', linewidth=2)\n",
    "    plt.axhline(y=0.80, color='r', linestyle='--', label='Minimum Threshold')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('ROC-AUC')\n",
    "    plt.title('Model Performance Over Time')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Alert if AUC drops below threshold for 3 consecutive days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete MLDC Workflow Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLDLCPipeline:\n",
    "    \"\"\"\n",
    "    Complete Machine Learning Development Life Cycle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_name):\n",
    "        self.project_name = project_name\n",
    "        self.preprocessor = None\n",
    "        self.model = None\n",
    "        \n",
    "    def frame_problem(self, business_goal, problem_type, success_metrics):\n",
    "        \"\"\"Phase 1: Frame Problem\"\"\"\n",
    "        self.problem_definition = {\n",
    "            'goal': business_goal,\n",
    "            'type': problem_type,\n",
    "            'metrics': success_metrics\n",
    "        }\n",
    "        print(f\"Problem framed: {business_goal}\")\n",
    "        \n",
    "    def gather_data(self, query):\n",
    "        \"\"\"Phase 2: Gather Data\"\"\"\n",
    "        self.raw_data = pd.read_sql(query, connection)\n",
    "        print(f\"Data gathered: {self.raw_data.shape}\")\n",
    "        \n",
    "    def process_data(self):\n",
    "        \"\"\"Phase 3: Clean & Process\"\"\"\n",
    "        self.cleaned_data = self.raw_data.dropna()\n",
    "        self.cleaned_data = self.cleaned_data.drop_duplicates()\n",
    "        print(f\"Data cleaned: {self.cleaned_data.shape}\")\n",
    "        \n",
    "    def eda(self):\n",
    "        \"\"\"Phase 4: Exploratory Analysis\"\"\"\n",
    "        print(self.cleaned_data.describe())\n",
    "        print(f\"Churn rate: {self.cleaned_data['churned'].mean():.2%}\")\n",
    "        \n",
    "    def engineer_features(self):\n",
    "        \"\"\"Phase 5: Feature Engineering\"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        \n",
    "        numeric_cols = self.cleaned_data.select_dtypes(include='number').columns\n",
    "        categorical_cols = self.cleaned_data.select_dtypes(include='object').columns\n",
    "        \n",
    "        numeric_transformer = StandardScaler()\n",
    "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_cols),\n",
    "                ('cat', categorical_transformer, categorical_cols)\n",
    "            ]\n",
    "        )\n",
    "        print(\"Features engineered\")\n",
    "        \n",
    "    def train_model(self):\n",
    "        \"\"\"Phase 6: Train & Select Model\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        X = self.cleaned_data.drop('churned', axis=1)\n",
    "        y = self.cleaned_data['churned']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        X_train_transformed = self.preprocessor.fit_transform(X_train)\n",
    "        X_test_transformed = self.preprocessor.transform(X_test)\n",
    "        \n",
    "        self.model = RandomForestClassifier(n_estimators=100)\n",
    "        self.model.fit(X_train_transformed, y_train)\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        score = roc_auc_score(y_test, self.model.predict_proba(X_test_transformed)[:, 1])\n",
    "        print(f\"Model trained. ROC-AUC: {score:.4f}\")\n",
    "        \n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def deploy_model(self):\n",
    "        \"\"\"Phase 7: Deploy\"\"\"\n",
    "        import joblib\n",
    "        joblib.dump(self.model, f'models/{self.project_name}_model.joblib')\n",
    "        joblib.dump(self.preprocessor, f'models/{self.project_name}_preprocessor.joblib')\n",
    "        print(\"Model deployed\")\n",
    "        \n",
    "    def monitor_model(self):\n",
    "        \"\"\"Phase 8 & 9: Monitor & Optimize\"\"\"\n",
    "        print(\"Monitoring pipeline active\")\n",
    "        print(\"Scheduled for weekly retraining\")\n",
    "\n",
    "# Usage\n",
    "pipeline = MLDLCPipeline(\"churn_prediction\")\n",
    "pipeline.frame_problem(\n",
    "    business_goal=\"Reduce churn by 5%\",\n",
    "    problem_type=\"Classification\",\n",
    "    success_metrics={'precision': 0.80, 'recall': 0.70}\n",
    ")\n",
    "pipeline.gather_data(\"SELECT * FROM customers\")\n",
    "pipeline.process_data()\n",
    "pipeline.eda()\n",
    "pipeline.engineer_features()\n",
    "pipeline.train_model()\n",
    "pipeline.deploy_model()\n",
    "pipeline.monitor_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used in Optimization\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| MLflow | Experiment tracking, model management |\n",
    "| Weights & Biases | Experiment tracking, visualization |\n",
    "| Apache Airflow | Workflow scheduling, orchestration |\n",
    "| Prefect | Data pipeline orchestration |\n",
    "| DVC | Data version control |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: MLDC Workflow\n",
    "\n",
    "| Phase | Goal | Key Activities | Tools |\n",
    "|-------|------|-----------------|-------|\n",
    "| 1. Frame Problem | Define clearly | Business goal, metrics, constraints | Docs, SQL |\n",
    "| 2. Gather Data | Collect raw data | Query databases, APIs, files | SQL, Pandas, APIs |\n",
    "| 3. Process Data | Clean & prepare | Remove nulls, duplicates, outliers | Pandas, Great Expectations |\n",
    "| 4. EDA | Understand data | Visualize, analyze distributions | Matplotlib, Seaborn, Pandas |\n",
    "| 5. Feature Engineering | Create meaningful features | Encode, scale, aggregate, interact | Scikit-learn, Pandas |\n",
    "| 6. Train & Select | Build & evaluate models | Train multiple models, cross-validate, tune | Scikit-learn, XGBoost |\n",
    "| 7. Deploy | Put in production | API, containers, serving platform | Docker, FastAPI, Kubernetes |\n",
    "| 8. Test & Monitor | Validate performance | Data validation, drift detection, A/B test | Great Expectations, Prometheus |\n",
    "| 9. Optimize | Continuous improvement | Retrain, analyze errors, update features | MLflow, Airflow, Monitoring tools |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Problem framing is 50% of success** - A well-defined problem is half-solved\n",
    "\n",
    "2. **Data quality > Model complexity** - Invest 80% time in data, 20% in algorithms\n",
    "\n",
    "3. **Always start simple** - Baseline models are fast to build and interpret\n",
    "\n",
    "4. **Cross-validate properly** - Prevents overfitting and ensures generalization\n",
    "\n",
    "5. **Monitor in production** - Models degrade over time; retraining is essential\n",
    "\n",
    "6. **Business metrics matter most** - Accuracy is not the goal; revenue/cost reduction is\n",
    "\n",
    "7. **Document everything** - Future you will thank present you\n",
    "\n",
    "8. **Iterate continuously** - ML is not one-time; it's ongoing optimization\n",
    "\n",
    "---\n",
    "\n",
    "**ML Development Life Cycle Complete. Ready to build production ML systems!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}