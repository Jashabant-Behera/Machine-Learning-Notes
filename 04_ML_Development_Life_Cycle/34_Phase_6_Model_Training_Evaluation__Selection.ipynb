{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Model Training, Evaluation & Selection\n",
    "\n",
    "### What is Model Training?\n",
    "\n",
    "Training is the process of feeding labeled data to an algorithm so it learns to make predictions.\n",
    "\n",
    "### Model Selection Strategy\n",
    "\n",
    "**1. Start Simple (Baseline Models)**\n",
    "\n",
    "Always start with simple models as baselines. This prevents over-engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Baseline: Logistic Regression\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "y_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Try Multiple Algorithms**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary of models to try\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics\n",
    "\n",
    "**For Classification Problems:**\n",
    "\n",
    "```\n",
    "1. Accuracy: (TP + TN) / Total\n",
    "   - Overall correctness\n",
    "   - Misleading with imbalanced data\n",
    "\n",
    "2. Precision: TP / (TP + FP)\n",
    "   - Of positive predictions, how many were correct?\n",
    "   - Important when false positives are costly\n",
    "   - Churn: Giving incentive to non-churners costs money\n",
    "\n",
    "3. Recall: TP / (TP + FN)\n",
    "   - Of actual positives, how many did we catch?\n",
    "   - Important when false negatives are costly\n",
    "   - Churn: Missing churners loses customers\n",
    "\n",
    "4. F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "   - Balanced combination of precision and recall\n",
    "   - Best when both matter equally\n",
    "\n",
    "5. ROC-AUC: Area Under Receiver Operating Characteristic Curve\n",
    "   - Measures model's ability to distinguish classes\n",
    "   - Ranges 0-1, higher is better\n",
    "   - Robust to class imbalance\n",
    "\n",
    "6. PR-AUC: Area Under Precision-Recall Curve\n",
    "   - Better for imbalanced datasets\n",
    "   - More informative than ROC-AUC for rare events\n",
    "```\n",
    "\n",
    "**Example: Interpreting Metrics for Churn Prediction**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "# [[TN  FP]\n",
    "#  [FN  TP]]\n",
    "\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['No Churn', 'Churned']))\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Business interpretation:\n",
    "# If Precision=80%: Of 100 customers we predict will churn, 80 actually will\n",
    "# If Recall=70%: Of all customers who actually churn, we catch 70%\n",
    "# Cost analysis: \n",
    "#   - Cost of retention offer (false positive): $50\n",
    "#   - Cost of losing customer (false negative): $500\n",
    "#   - Better to have high recall (catch churners) than high precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Regression Problems:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# MAE: Mean Absolute Error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MAE: {mae}\")  # Average absolute error in same units as target\n",
    "\n",
    "# RMSE: Root Mean Squared Error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")  # Penalizes larger errors more\n",
    "\n",
    "# MAPE: Mean Absolute Percentage Error\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f\"MAPE: {mape}%\")  # Percentage error (good for comparison)\n",
    "\n",
    "# R-squared: Coefficient of Determination\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R\u00b2: {r2}\")  # Proportion of variance explained (0-1, higher better)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation (Robust Evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# k-Fold Cross-Validation\n",
    "# Splits data into k folds, trains k times, evaluates on each fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Get scores for each fold\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.4f}\")\n",
    "print(f\"Std: {cv_scores.std():.4f}\")\n",
    "\n",
    "# If mean=0.82 and std=0.03, model is stable and generalizes well\n",
    "# If mean=0.85 and std=0.12, high variance suggests overfitting risk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Churn dataset has 85% no-churn, 15% churn\n",
    "# Simple accuracy becomes misleading (predicting all \"no-churn\" gives 85%)\n",
    "\n",
    "# Solution 1: Class Weights\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "# Automatically gives more importance to minority class\n",
    "\n",
    "# Solution 2: SMOTE (Synthetic Minority Oversampling)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "# Artificially creates synthetic minority examples\n",
    "\n",
    "# Solution 3: Threshold Adjustment\n",
    "# Instead of predicting class if prob > 0.5, use custom threshold\n",
    "y_pred_custom = (y_proba > 0.3).astype(int)  # Lower threshold catches more churners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Grid Search: Try all combinations\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Random Search: Try random combinations (faster for large spaces)\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 301, 10),\n",
    "    'max_depth': np.arange(3, 20),\n",
    "    'min_samples_split': np.arange(2, 11)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison & Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs tuned model\n",
    "baseline_auc = roc_auc_score(y_test, baseline_model.predict_proba(X_test)[:, 1])\n",
    "tuned_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"Baseline ROC-AUC: {baseline_auc:.4f}\")\n",
    "print(f\"Tuned ROC-AUC: {tuned_auc:.4f}\")\n",
    "print(f\"Improvement: {(tuned_auc - baseline_auc):.4f}\")\n",
    "\n",
    "# Select model based on:\n",
    "# 1. Performance metrics (ROC-AUC, F1, precision/recall trade-off)\n",
    "# 2. Latency requirements (simpler models are faster)\n",
    "# 3. Interpretability needs (tree models are more interpretable than neural nets)\n",
    "# 4. Complexity (don't overfit with overly complex models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used in Model Training\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| Scikit-learn | ML algorithms, evaluation |\n",
    "| XGBoost | Gradient boosting (fast, accurate) |\n",
    "| LightGBM | Faster gradient boosting |\n",
    "| CatBoost | Handles categorical features well |\n",
    "| TensorFlow/PyTorch | Deep learning |\n",
    "| Hyperopt | Bayesian hyperparameter tuning |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}