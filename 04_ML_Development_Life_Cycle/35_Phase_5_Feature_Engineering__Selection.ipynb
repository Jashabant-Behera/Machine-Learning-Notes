{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Feature Engineering & Selection\n",
    "\n",
    "### What is Feature Engineering?\n",
    "\n",
    "Feature engineering is the process of transforming raw data into meaningful features that help the model learn better patterns.\n",
    "\n",
    "\"Feature engineering is the most important part of machine learning\" - Andrew Ng\n",
    "\n",
    "### Why Feature Engineering Matters\n",
    "\n",
    "```\n",
    "Good Features + Simple Model = Often beats Poor Features + Complex Model\n",
    "\n",
    "Example:\n",
    "Bad approach: Raw transaction amount \u2192 Model accuracy 72%\n",
    "Good approach: Customer_avg_transaction / customer_total_spent \u2192 Accuracy 85%\n",
    "\n",
    "The ratio (normalized feature) is more informative than raw amount.\n",
    "```\n",
    "\n",
    "### Feature Engineering Techniques\n",
    "\n",
    "**1. Encoding Categorical Variables**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "df = cleaned_data.copy()\n",
    "\n",
    "# Technique 1: Label Encoding (for ordinal categories)\n",
    "# Use when: Categories have order (Low, Medium, High)\n",
    "le = LabelEncoder()\n",
    "df['risk_level_encoded'] = le.fit_transform(df['risk_level'])\n",
    "# Low=0, Medium=1, High=2\n",
    "\n",
    "# Technique 2: One-Hot Encoding (for nominal categories)\n",
    "# Use when: Categories have no order (USA, UK, Canada)\n",
    "account_type_dummies = pd.get_dummies(df['account_type'], prefix='account_type')\n",
    "df = pd.concat([df, account_type_dummies], axis=1)\n",
    "# Creates: account_type_basic, account_type_premium, account_type_vip\n",
    "\n",
    "# Technique 3: Target Encoding (for high-cardinality)\n",
    "# Use when: Many categories (100+ cities), need to reduce dimensionality\n",
    "target_encoding = df.groupby('city')['churned'].mean()\n",
    "df['city_churn_rate'] = df['city'].map(target_encoding)\n",
    "# Each city replaced by its actual churn rate\n",
    "\n",
    "# Technique 4: Frequency Encoding\n",
    "# Use when: Frequency of category is informative\n",
    "frequency_encoding = df['country'].value_counts() / len(df)\n",
    "df['country_frequency'] = df['country'].map(frequency_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Scaling/Normalization Numeric Features**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Technique 1: Standardization (z-score normalization)\n",
    "# Formula: x_scaled = (x - mean) / std\n",
    "# Use when: Features are normally distributed, algorithms assume normal dist\n",
    "scaler = StandardScaler()\n",
    "df['age_scaled'] = scaler.fit_transform(df[['age']])\n",
    "# Result: mean=0, std=1\n",
    "\n",
    "# Technique 2: Min-Max Scaling\n",
    "# Formula: x_scaled = (x - min) / (max - min)\n",
    "# Use when: Need features in fixed range [0, 1]\n",
    "minmax = MinMaxScaler()\n",
    "df['total_spent_scaled'] = minmax.fit_transform(df[['total_spent']])\n",
    "# Result: range [0, 1]\n",
    "\n",
    "# Technique 3: Robust Scaling (handles outliers better)\n",
    "# Use when: Data has outliers, don't want to remove them\n",
    "robust = RobustScaler()\n",
    "df['age_robust'] = robust.fit_transform(df[['age']])\n",
    "# Uses median and IQR instead of mean and std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Creating Time-Based Features**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract date features\n",
    "df['account_creation_date'] = pd.to_datetime(df['account_creation_date'])\n",
    "df['last_transaction_date'] = pd.to_datetime(df['last_transaction_date'])\n",
    "\n",
    "# Temporal features\n",
    "df['account_age_days'] = (pd.Timestamp.now() - df['account_creation_date']).dt.days\n",
    "df['days_since_last_transaction'] = (pd.Timestamp.now() - df['last_transaction_date']).dt.days\n",
    "\n",
    "# Cyclic features\n",
    "df['account_creation_month'] = df['account_creation_date'].dt.month\n",
    "df['account_creation_quarter'] = df['account_creation_date'].dt.quarter\n",
    "df['account_creation_dayofweek'] = df['account_creation_date'].dt.dayofweek\n",
    "\n",
    "# Cyclical encoding (for month: 1-12 should wrap around)\n",
    "# Use sine/cosine transformation to maintain cyclical nature\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['account_creation_month']/12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['account_creation_month']/12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Creating Aggregation Features**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer-level aggregations (RFM: Recency, Frequency, Monetary)\n",
    "\n",
    "# Recency: Days since last purchase\n",
    "df['recency'] = (df['observation_date'] - df['last_purchase_date']).dt.days\n",
    "\n",
    "# Frequency: Number of purchases\n",
    "df['frequency'] = df.groupby('customer_id')['transaction_id'].transform('count')\n",
    "\n",
    "# Monetary: Total amount spent\n",
    "df['monetary'] = df.groupby('customer_id')['transaction_amount'].transform('sum')\n",
    "\n",
    "# Additional aggregations\n",
    "df['avg_transaction_amount'] = df.groupby('customer_id')['transaction_amount'].transform('mean')\n",
    "df['std_transaction_amount'] = df.groupby('customer_id')['transaction_amount'].transform('std')\n",
    "df['max_transaction_amount'] = df.groupby('customer_id')['transaction_amount'].transform('max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Creating Interaction Features**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction between two features (useful when features interact)\n",
    "\n",
    "# Ratio features\n",
    "df['debt_to_income_ratio'] = df['total_debt'] / df['annual_income']\n",
    "df['avg_transaction_to_total'] = df['avg_transaction_amount'] / df['total_spent']\n",
    "\n",
    "# Product features\n",
    "df['age_by_account_age'] = df['age'] * df['account_age_days']\n",
    "\n",
    "# Polynomial features\n",
    "df['age_squared'] = df['age'] ** 2\n",
    "df['total_spent_log'] = np.log1p(df['total_spent'])  # log transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Feature Selection**\n",
    "\n",
    "### Why Select Features?\n",
    "\n",
    "- Reduce dimensionality (fewer features = faster training)\n",
    "- Remove noise (irrelevant features confuse model)\n",
    "- Improve interpretability (easier to explain)\n",
    "- Reduce overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Method 1: Statistical Tests (Fast)\n",
    "# SelectKBest selects k features with highest statistical scores\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Method 2: Mutual Information (captures non-linear relationships)\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Method 3: Feature Importance from Tree Models\n",
    "# Most practical for real-world use\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "# Keep top 15 features (example)\n",
    "top_features = feature_importance.head(15)['feature'].tolist()\n",
    "\n",
    "# Method 4: Recursive Feature Elimination (Wrapper Method)\n",
    "# Iteratively removes least important features\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=15)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "selected_rfe = X.columns[rfe.support_]\n",
    "\n",
    "# Method 5: Correlation-based (Remove multicollinearity)\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "# Find features with correlation > 0.9\n",
    "drop_features = [column for column in upper_triangle.columns \n",
    "                 if any(upper_triangle[column] > 0.9)]\n",
    "X_clean = X.drop(columns=drop_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Feature Engineering Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def create_feature_engineering_pipeline():\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Define columns\n",
    "    numeric_features = ['age', 'total_spent', 'transaction_count', \n",
    "                       'account_age_days']\n",
    "    categorical_features = ['account_type', 'country']\n",
    "    \n",
    "    # Numeric transformer: Scale\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Categorical transformer: One-hot encode\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "# Use in model training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor = create_feature_engineering_pipeline()\n",
    "\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used in Feature Engineering\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| Pandas | Feature creation, manipulation |\n",
    "| Scikit-learn | Feature selection, scaling |\n",
    "| Featuretools | Automated feature engineering |\n",
    "| Category Encoders | Advanced categorical encoding |\n",
    "| Apache Spark | Large-scale feature engineering |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}