{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Data Processing & Cleaning\n",
    "\n",
    "### Why is Data Cleaning Critical?\n",
    "\n",
    "\"Garbage in, garbage out\" - Data quality directly impacts model quality.\n",
    "\n",
    "According to research, data scientists spend 70-80% of time on data cleaning and preparation.\n",
    "\n",
    "### Common Data Quality Issues\n",
    "\n",
    "**1. Missing Values**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = raw_data.copy()\n",
    "\n",
    "# Check missing values\n",
    "print(df.isnull().sum())\n",
    "print(df.isnull().sum() / len(df) * 100)  # Percentage\n",
    "\n",
    "# Output example:\n",
    "# transaction_count    0 (0.0%)\n",
    "# total_spent          0 (0.0%)\n",
    "# age                  542 (5.4%)\n",
    "# support_tickets      2104 (21%)\n",
    "# last_support_date    2104 (21%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategies for Handling Missing Values:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Drop rows (only if <5% missing)\n",
    "df_dropped = df.dropna(subset=['age'])\n",
    "\n",
    "# Strategy 2: Drop columns (only if >50% missing)\n",
    "df = df.drop(columns=['last_support_date'])  # 21% missing, might drop\n",
    "\n",
    "# Strategy 3: Imputation with mean/median (numeric)\n",
    "df['age'].fillna(df['age'].median(), inplace=True)\n",
    "\n",
    "# Strategy 4: Imputation with mode (categorical)\n",
    "df['country'].fillna(df['country'].mode()[0], inplace=True)\n",
    "\n",
    "# Strategy 5: Forward fill / Backward fill (time series)\n",
    "df['balance'] = df['balance'].fillna(method='ffill')\n",
    "\n",
    "# Strategy 6: Advanced imputation (ML-based)\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Duplicate Records**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "print(df.duplicated().sum())  # Exact duplicates\n",
    "\n",
    "# Check duplicates on specific columns\n",
    "print(df.duplicated(subset=['customer_id']).sum())\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "df = df.drop_duplicates(subset=['customer_id'], keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Data Type Issues**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify type issues\n",
    "print(df.dtypes)\n",
    "\n",
    "# Example: Age column stored as string \"25\" instead of int\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "# Convert dates\n",
    "df['account_creation_date'] = pd.to_datetime(df['account_creation_date'], \n",
    "                                             errors='coerce')\n",
    "\n",
    "# Convert categorical\n",
    "df['account_type'] = df['account_type'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Inconsistent/Invalid Values**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ranges\n",
    "print(df['age'].describe())\n",
    "\n",
    "# Example: Age > 150 is unrealistic\n",
    "df = df[df['age'] <= 120]\n",
    "\n",
    "# Example: Transaction amount < 0 is invalid\n",
    "df = df[df['transaction_amount'] > 0]\n",
    "\n",
    "# Case inconsistency in categorical\n",
    "df['country'] = df['country'].str.upper()\n",
    "\n",
    "# Whitespace issues\n",
    "df['country'] = df['country'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Outliers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Statistical (IQR)\n",
    "Q1 = df['transaction_amount'].quantile(0.25)\n",
    "Q3 = df['transaction_amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df['transaction_amount'] < Q1 - 1.5*IQR) | \n",
    "              (df['transaction_amount'] > Q3 + 1.5*IQR)]\n",
    "\n",
    "# Method 2: Z-score\n",
    "from scipy import stats\n",
    "z_scores = np.abs(stats.zscore(df['transaction_amount']))\n",
    "outliers = df[z_scores > 3]\n",
    "\n",
    "# Method 3: Isolation Forest (ML-based anomaly detection)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.05)\n",
    "outliers = df[iso_forest.fit_predict(df[['transaction_amount']]) == -1]\n",
    "\n",
    "# Decision: Keep or Remove? \n",
    "# For fraud detection: KEEP (outliers might be fraudulent)\n",
    "# For customer churn: REMOVE (extreme cases might distort pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Data Cleaning Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Complete data cleaning pipeline\n",
    "    \"\"\"\n",
    "    # Step 1: Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "    \n",
    "    # Step 2: Type conversion\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "    df['account_creation_date'] = pd.to_datetime(df['account_creation_date'])\n",
    "    \n",
    "    # Step 3: Handle missing values\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Step 4: Handle outliers (remove extreme cases)\n",
    "    df = df[df['age'] <= 120]\n",
    "    df = df[df['transaction_amount'] > 0]\n",
    "    \n",
    "    # Step 5: Standardize formats\n",
    "    df['country'] = df['country'].str.upper().str.strip()\n",
    "    \n",
    "    # Step 6: Log cleaning report\n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "cleaned_data = clean_data(raw_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used in Data Cleaning\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| Pandas | Data manipulation, cleaning |\n",
    "| NumPy | Numerical operations |\n",
    "| Great Expectations | Data validation & profiling |\n",
    "| Deequ | Large-scale data quality |\n",
    "| OpenRefine | Visual data cleaning |\n",
    "| Apache Spark | Distributed data cleaning |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}