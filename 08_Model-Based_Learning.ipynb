{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model-Based Learning\n",
    "\n",
    "### Definition\n",
    "**Model-Based Learning** (also called **Parametric Learning** or **Eager Learning**) creates a **mathematical model** that captures patterns in training data. The learning happens during training; predictions are just applying the learned model.\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "```\n",
    "Training Phase:\n",
    "Input Data \u2192 Learn Parameters/Structure \u2192 Create Mathematical Model\n",
    "(Intensive computation, learns general patterns)\n",
    "\n",
    "Prediction Phase:\n",
    "New Sample \u2192 Apply Model (simple calculation) \u2192 Prediction\n",
    "(Fast, uses learned knowledge)\n",
    "```\n",
    "\n",
    "### How Model-Based Learning Works\n",
    "\n",
    "The algorithm learns a **parameterized function** that generalizes to new data:\n",
    "\n",
    "```\n",
    "f(x) = \u03b8\u2080 + \u03b8\u2081\u00d7x\u2081 + \u03b8\u2082\u00d7x\u2082 + ... (Linear Regression)\n",
    "       where \u03b8 are learned parameters\n",
    "\n",
    "f(x) = Tree of decision rules (Decision Trees)\n",
    "       where rules are learned\n",
    "\n",
    "f(x) = Deep Neural Network with billions of parameters\n",
    "       where parameters learned via backpropagation\n",
    "```\n",
    "\n",
    "### Common Model-Based Algorithms\n",
    "\n",
    "#### 1. **Linear Regression** (Parametric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Training: Learn weights \u03b8\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # Learns: y = 1.3 + 0.5*x (approximately)\n",
    "\n",
    "print(f\"Learned parameters (coefficients): {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# Prediction: Just apply the model\n",
    "x_new = np.array([[6]])\n",
    "y_pred = model.predict(x_new)\n",
    "print(f\"Prediction for x=6: {y_pred[0]:.2f}\")\n",
    "\n",
    "# Model formula: y = 0.5 * x + 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Decision Trees** (Non-parametric but model-based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Training: Learn decision rules\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X, y)  # Learns tree structure\n",
    "\n",
    "# Visualize learned tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(tree, feature_names=iris.feature_names,\n",
    "          class_names=iris.target_names, filled=True)\n",
    "plt.title('Learned Decision Tree Model')\n",
    "plt.show()\n",
    "\n",
    "# Prediction: Follow decision paths\n",
    "x_new = [[5.1, 3.5, 1.4, 0.2]]\n",
    "y_pred = tree.predict(x_new)\n",
    "print(f\"Prediction: {iris.target_names[y_pred[0]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Neural Networks** (Parametric with millions of parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create data\n",
    "X, y = make_classification(n_samples=1000, n_features=20,\n",
    "                           n_informative=10, n_classes=2)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Training: Learn millions of weights\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=200)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Model structure\n",
    "print(f\"Number of parameters: {model.n_parameters_}\")\n",
    "print(f\"Layer sizes: Input->100->50->Output\")\n",
    "\n",
    "# Prediction: Forward pass through network\n",
    "y_pred = model.predict(X[:10])\n",
    "print(f\"Predictions: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Model-Based Learning\n",
    "\n",
    "#### 1. **Parametric** \ud83d\udcd0\n",
    "- Creates a **fixed-size** model regardless of data size\n",
    "- Number of parameters determined before training\n",
    "- Generalizes through learned parameters\n",
    "- Simpler models (fewer parameters) generalize better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model: 100 samples = 1M samples, same parameters\n",
    "# Model size doesn't grow with data size\n",
    "\n",
    "X_100 = np.random.rand(100, 20)\n",
    "X_1M = np.random.rand(1_000_000, 20)\n",
    "\n",
    "model_100 = LinearRegression()\n",
    "model_100.fit(X_100, np.random.rand(100))\n",
    "\n",
    "model_1M = LinearRegression()\n",
    "model_1M.fit(X_1M, np.random.rand(1_000_000))\n",
    "\n",
    "# Both models have same number of parameters!\n",
    "print(f\"Model trained on 100 samples: {model_100.coef_.shape}\")\n",
    "print(f\"Model trained on 1M samples: {model_1M.coef_.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Eager Learning** \ud83c\udfaf\n",
    "- Learning happens during training (expensive computation)\n",
    "- Predictions are fast (apply learned model)\n",
    "- Training phase is computationally intensive\n",
    "- Prediction phase is lightweight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Training: Computationally expensive\n",
    "X_train = np.random.rand(50000, 100)\n",
    "y_train = np.random.rand(50000)\n",
    "\n",
    "start = time.time()\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # Solve normal equations: (X'X)^-1 X'y\n",
    "train_time = time.time() - start\n",
    "print(f\"Training time: {train_time:.4f} seconds\")\n",
    "\n",
    "# Prediction: Very fast\n",
    "X_test = np.random.rand(1000, 100)\n",
    "start = time.time()\n",
    "y_pred = model.predict(X_test)  # Simple matrix multiplication\n",
    "predict_time = time.time() - start\n",
    "print(f\"Prediction time: {predict_time:.4f} seconds\")\n",
    "print(f\"Training is {train_time/predict_time:.0f}x slower than prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Generalization**\n",
    "- Learned parameters generalize to unseen data\n",
    "- Avoid overfitting through regularization\n",
    "- Need sufficient training data\n",
    "- Requires careful model selection\n",
    "\n",
    "### Advantages of Model-Based Learning\n",
    "\n",
    "#### 1. **Fast Predictions** \u26a1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once model learned, predictions are extremely fast\n",
    "# No distance calculations like KNN\n",
    "# Just matrix operations (very optimized)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "X_train = np.random.rand(100000, 50)\n",
    "y_train = np.random.randint(0, 2, 100000)\n",
    "\n",
    "knn.fit(X_train, y_train)  # Stores 100k samples\n",
    "tree.fit(X_train, y_train)  # Learns small tree\n",
    "\n",
    "X_test = np.random.rand(1000, 50)\n",
    "\n",
    "start = time.time()\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "tree_pred = tree.predict(X_test)\n",
    "tree_time = time.time() - start\n",
    "\n",
    "print(f\"KNN prediction time: {knn_time:.4f}s\")\n",
    "print(f\"Tree prediction time: {tree_time:.4f}s\")\n",
    "print(f\"Tree is {knn_time/tree_time:.0f}x faster\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Memory Efficient** \ud83d\udcbe\n",
    "- Model size independent of training data size\n",
    "- Linear model with 1M parameters works with any dataset size\n",
    "- Suitable for deployment (small model files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Model-based: Tiny model file\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model_size = len(pickle.dumps(model)) / 1024\n",
    "print(f\"Linear model size: {model_size:.2f} KB\")\n",
    "\n",
    "# Instance-based: Huge model file (stores all data!)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_size = len(pickle.dumps(knn)) / (1024**2)\n",
    "print(f\"KNN model size: {knn_size:.2f} MB\")\n",
    "# KNN is 1000s of times larger!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Scalable Deployment** \ud83d\ude80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-based models easy to deploy\n",
    "# Just save parameters, load in production\n",
    "\n",
    "# Save trained model\n",
    "import joblib\n",
    "joblib.dump(model, 'trained_model.pkl')\n",
    "\n",
    "# Load in production\n",
    "from joblib import load\n",
    "model_prod = load('trained_model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model_prod.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Interpretability** \ud83d\udd0d\n",
    "- Linear models: See weight importance\n",
    "- Decision trees: Understand decision rules\n",
    "- More understandable than instance-based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model: See feature importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance = coefficients\n",
    "feature_importance = model.coef_[0]\n",
    "feature_names = ['feature_1', 'feature_2', ..., 'feature_n']\n",
    "\n",
    "for name, importance in zip(feature_names, feature_importance):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "# Decision tree: Visually interpretable\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(tree, feature_names=feature_names, filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Handles Infinite Data** \u267e\ufe0f\n",
    "- Doesn't need to store all data\n",
    "- Works with streaming data (incremental learning)\n",
    "- Scalable to unlimited dataset sizes\n",
    "\n",
    "### Disadvantages of Model-Based Learning\n",
    "\n",
    "#### 1. **Assumption-Dependent** \u26a0\ufe0f\n",
    "- Makes assumptions about data (linear, gaussian, etc.)\n",
    "- Wrong assumptions lead to poor performance\n",
    "- Assumptions may not match reality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Linear regression assumes linear relationship\n",
    "# If data is nonlinear, performance suffers\n",
    "\n",
    "# Generate nonlinear data\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Linear model assumes straight line\n",
    "linear = LinearRegression()\n",
    "linear.fit(X, y)\n",
    "linear_pred = linear.predict(X)\n",
    "linear_mse = np.mean((linear_pred - y) ** 2)\n",
    "\n",
    "# Polynomial model (still linear model but higher degree)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=10)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "poly = LinearRegression()\n",
    "poly.fit(X_poly, y)\n",
    "poly_pred = poly.predict(X_poly)\n",
    "poly_mse = np.mean((poly_pred - y) ** 2)\n",
    "\n",
    "print(f\"Linear regression MSE: {linear_mse:.4f}\")\n",
    "print(f\"Polynomial (degree 10) MSE: {poly_mse:.4f}\")\n",
    "# Polynomial much better because it doesn't assume linearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Slower Learning** \ud83d\udc0c\n",
    "- Training computationally expensive\n",
    "- Fitting parameters takes time\n",
    "- Not suitable when quick updates needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training is expensive for complex models\n",
    "X = np.random.rand(100000, 100)\n",
    "y = np.random.rand(100000)\n",
    "\n",
    "# Simple model: Fast\n",
    "start = time.time()\n",
    "linear = LinearRegression()\n",
    "linear.fit(X, y)\n",
    "print(f\"Linear model training: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Complex model: Slow\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "start = time.time()\n",
    "forest = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "forest.fit(X, y)\n",
    "print(f\"Random Forest training: {time.time() - start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Requires Sufficient Training Data**\n",
    "- Need enough data to learn parameters well\n",
    "- Too little data \u2192 overfitting\n",
    "- Too much noise \u2192 poor generalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model needs sufficient training data\n",
    "sample_sizes = [10, 50, 100, 500, 1000]\n",
    "accuracies = []\n",
    "\n",
    "for size in sample_sizes:\n",
    "    X_temp = np.random.rand(size, 20)\n",
    "    y_temp = np.random.randint(0, 2, size)\n",
    "    \n",
    "    X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.3)\n",
    "    \n",
    "    model_t = RandomForestClassifier(n_estimators=10)\n",
    "    model_t.fit(X_train_t, y_train_t)\n",
    "    acc = model_t.score(X_test_t, y_test_t)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, accuracies, marker='o', linewidth=2)\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model-Based Learning: Accuracy vs Data Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy improves with more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Hyperparameter Tuning**\n",
    "- Many models require tuning (hidden layers, regularization, etc.)\n",
    "- Wrong hyperparameters \u2192 poor performance\n",
    "- Grid search expensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning required\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Many parameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# This becomes computationally expensive!\n",
    "rf = RandomForestClassifier()\n",
    "grid = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\n",
    "# Fits many models trying all combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Model-Based Learning\n",
    "\n",
    "\u2705 **Use Model-Based when:**\n",
    "- Large datasets available\n",
    "- Need fast predictions\n",
    "- Storage/memory important\n",
    "- Patterns are stable\n",
    "- Can afford training time\n",
    "- Need interpretable models\n",
    "- Streaming data with concept drift\n",
    "\n",
    "\u274c **Avoid when:**\n",
    "- Very little training data\n",
    "- Instant adaptation needed\n",
    "- Data patterns extremely complex\n",
    "- Cannot afford training time\n",
    "- Unknown pattern complexity\n",
    "\n",
    "### Real-World Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 1: Housing Price Prediction\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "house_features = [\n",
    "    'square_feet', 'bedrooms', 'bathrooms', 'age', 'location'\n",
    "]\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train_houses, y_train_prices)\n",
    "\n",
    "# Fast prediction on new house\n",
    "new_house_pred = model.predict([[2000, 3, 2, 10, 'suburban']])\n",
    "\n",
    "# Application 2: Disease Diagnosis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Patient symptoms as features\n",
    "patient_symptoms = [fever, cough, fatigue, headache]\n",
    "diagnosis_model = RandomForestClassifier()\n",
    "diagnosis_model.fit(X_train_symptoms, y_train_diagnosis)\n",
    "\n",
    "# Predict disease\n",
    "predicted_disease = diagnosis_model.predict([patient_symptoms])\n",
    "\n",
    "# Application 3: Spam Detection\n",
    "email_features = [sender_reputation, content_similarity, link_count]\n",
    "spam_model = LogisticRegression()\n",
    "spam_model.fit(X_train_emails, y_train_spam_labels)\n",
    "\n",
    "# Classify incoming email\n",
    "is_spam = spam_model.predict([incoming_email_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}